{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9d1ab3-9092-4f24-9d38-159cdb8ab28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_NAME'] = \"/opt/spark\"\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "# os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/spark/work-dir/.venv/bin/python3'\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/spark/work-dir/.venv/bin/python3'\n",
    "\n",
    "# można też spróbować wykorzystać moduł findspark do automatycznego odnalezienia miejsca instalacji sparka\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# lub\n",
    "# findspark.init(\"/opt/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e9131dc-85a7-4815-bcb0-971b72602934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/13 12:35:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://f641dfb305eb:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Create-DataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Create-DataFrame>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder.master(\"spark://spark-master:7077\").appName(\"Create-DataFrame\").getOrCreate()\n",
    "# konfiguracja z określeniem liczby wątków (2) oraz ilości pamięci do wykorzystania poza stertą interpretera Pythona\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Create-DataFrame\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"4g\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5d974c-9c1c-4dbc-9173-170d01a07b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba partycji: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierwszy element: 0\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# lista wartości zostaje podzielona na partycje i rozproszona na wszystkie dostępne węzły\n",
    "# https://spark.apache.org/docs/3.5.3/api/python/reference/api/pyspark.SparkContext.parallelize.html\n",
    "rdd = spark.sparkContext.parallelize(list(range(20)))\n",
    "\n",
    "# rdd w formie rozproszonej zostaje scalone w listę zawierającą wszystkie elementy RDD\n",
    "# np. za pomocą funkcji collect()\n",
    "# https://spark.apache.org/docs/3.5.3/api/python/reference/api/pyspark.RDD.collect.html\n",
    "\n",
    "rddCollect = rdd.collect()\n",
    "display(type(rddCollect))\n",
    "print(f\"Liczba partycji: {rdd.getNumPartitions()}\")\n",
    "print(f\"Pierwszy element: {rdd.first()}\")\n",
    "print(rddCollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a17821-16a7-4692-b125-5b284b0e82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obiekt RDD może przechowywać dane z różnych źródeł, które są zgodne z systemem plików Apache Hadoop\n",
    "# np. Amazon S3, Cassandra, HDFS, HBase i inne\n",
    "\n",
    "# możemy dla uniknięcia potrzeby każdorazowego odwoływania się do kontekstu poprzez spark.sparkContext zapisać sobie to w zmiennej pomocniczej\n",
    "sc = spark.sparkContext\n",
    "# tutaj wczytamy do RDD plik tekstowy\n",
    "pan_tadeusz_file = sc.textFile(\"pan-tadeusz.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "654eb0ca-82a1-47fd-871d-e32405819574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Adam Mickiewicz',\n",
       " '',\n",
       " 'Pan Tadeusz',\n",
       " 'czyli ostatni zajazd na Litwie',\n",
       " '',\n",
       " 'ISBN 978-83-288-2495-9',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(pan_tadeusz_file.getNumPartitions())\n",
    "pan_tadeusz_file.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8d3a1f3-5b4b-4294-8ffd-3e95441725d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# możemy zmienić liczbę automatycznie stworzonych partycji i ponownie rozproszyć je po węzłach\n",
    "pan_tadeusz_file = pan_tadeusz_file.repartition(4)\n",
    "pan_tadeusz_file.getNumPartitions()\n",
    "\n",
    "# również metoda coalesce może posłużyć nam do zmiany ilości partycji dla obiektu RDD np. po zastosowaniu filtrowania, które\n",
    "# znacznie zmniejsza wielkość pierwotnego obiektu RDD a co za tym idzie każdej partycji i dalsze obliczenia mogą nie być\n",
    "# wykonywane zbyt efektywnie (zbyt mały rozmiar partycji)\n",
    "# https://spark.apache.org/docs/3.5.3/api/python/reference/api/pyspark.RDD.coalesce.html\n",
    "# główna różnica między repartition a coalesce jest taka, że ta pierwsza wykorzystuje mechanizm tasowania danych a ta druga może, ale nie\n",
    "# musi go wykorzystywać gdyż możemy tym sterować za pomocą parametru wywołania tej metody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0706726f-d5a7-4bac-a1ed-28dde267a8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 2) / 2]17129\n",
      "17366\n",
      "17307\n",
      "17293\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# jedną z funkcji dostępnej w tym API jest możliwość wykonania funkcji na każdej z partycji\n",
    "# minusem może być to, że funkcja foreachPartition zwraca typ None, więc wyniki należy przetworzyć w inny sposób\n",
    "\n",
    "def count_words(iterator):\n",
    "    words = sum([len(x.split()) for x in iterator])\n",
    "    print(words)\n",
    "\n",
    "pan_tadeusz_file.foreachPartition(count_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b86372f-f46b-4463-a78d-c6a675369edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69095"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# funkcje map oraz reduce\n",
    "\n",
    "# możemy również wykonać operację w inny sposób, tym razem mapując funkcję na każdy element obiektu RDD\n",
    "# zwrócony zostanie obiekt RDD, na którym możemy wykonać kolejne operacje\n",
    "pan_tadeusz_file.map(lambda s: len(s.split())).take(10)\n",
    "\n",
    "# np. reduce - i tu nawiązanie do dość znanej techniki przetwarzania, MapReduce\n",
    "# więcej: https://en.wikipedia.org/wiki/MapReduce\n",
    "# oraz: https://wiadrodanych.pl/big-data/jak-dziala-mapreduce/\n",
    "pan_tadeusz_file.map(lambda s: len(s.split())).reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf68cc06-73bf-4568-b525-eb36c9908306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69095"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lub tak - ten sam efekt\n",
    "from operator import add\n",
    "pan_tadeusz_file.map(lambda s: len(s.split())).reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00f5c3b8-0b74-4503-8eca-24195e2a05b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'zaraz', 'mogłem', 'pieszo,', 'do', 'Twych', 'świątyń', 'progu'],\n",
       " ['Iść', 'za', 'wrócone', 'życie', 'podziękować', 'Bogu),'],\n",
       " ['Tak', 'nas', 'powrócisz', 'cudem', 'na', 'Ojczyzny', 'łono.'],\n",
       " ['Tymczasem', 'przenoś', 'moją', 'duszę', 'utęsknioną'],\n",
       " ['Do', 'tych', 'pagórków', 'leśnych,', 'do', 'tych', 'łąk', 'zielonych,'],\n",
       " ['Szeroko', 'nad', 'błękitnym', 'Niemnem', 'rozciągnionych;'],\n",
       " ['Do', 'tych', 'pól', 'malowanych', 'zbożem', 'rozmaitem,'],\n",
       " ['Wyzłacanych', 'pszenicą,', 'posrebrzanych', 'żytem;'],\n",
       " ['Gdzie', 'bursztynowy', 'świerzop,', 'gryka', 'jak', 'śnieg', 'biała,'],\n",
       " ['Gdzie', 'panieńskim', 'rumieńcem', 'dzięcielina', 'pała,']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'zaraz',\n",
       " 'mogłem',\n",
       " 'pieszo,',\n",
       " 'do',\n",
       " 'Twych',\n",
       " 'świątyń',\n",
       " 'progu',\n",
       " 'Iść',\n",
       " 'za']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# różnica między map() a flatMap() dla tego przypadku\n",
    "display(pan_tadeusz_file.map(lambda s: s.split()).take(10))\n",
    "pan_tadeusz_file.flatMap(lambda s: s.split()).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f88d46-7a57-4bad-8a84-8488e6b0c3b2",
   "metadata": {},
   "source": [
    "# Zadanie 1\n",
    "Wykorzystując plik z treścią Pana Tadeusza zaprezentowany w przykładach policz ile jest linii, w których zawiera się słowo Tadeusz. Wykorzystaj akcje i transformacje RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5985a8c-e874-4315-a366-70b61f48d2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba linii zawierających słowo 'Tadeusz': 182\n"
     ]
    }
   ],
   "source": [
    "tadeusz_lines = pan_tadeusz_file.filter(lambda line: \"Tadeusz\" in line)\n",
    "count_tadeusz_lines = tadeusz_lines.count()\n",
    "\n",
    "print(f\"Liczba linii zawierających słowo 'Tadeusz': {count_tadeusz_lines}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4649fb7-d9b2-46bd-b09e-85d9d363bc3a",
   "metadata": {},
   "source": [
    "# Zadanie 2\n",
    "Wykorzystując metodę top() dla obiektu RDD wyświetl 3 najdłuższe linie wczytane z pliku z książką Pan Tadeusz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb1e5235-9bab-40da-82b2-2ce4e770c5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trzy najdłuższe linie w pliku:\n",
      "1. O niebezpieczeństwach wynikających z nieporządnego obozowania — Odsiecz niespodziana — Smutne położenie szlachty — Odwiedziny kwestarskie są wróżbą ratunku — Major Płut zbytnią zalotnością ściąga na siebie burzę — Wystrzał z krócicy, hasło boju — Czyny Kropiciela, czyny i niebezpieczeństwa Maćka — Konewka zasadzką ocala Soplicowo — Posiłki jezdne, atak na piechotę — Czyny Tadeusza — Pojedynek dowódców przerwany zdradą — Wojski stanowczym manewrem przechyla szalę boju — Czyny krwawe Gerwazego — Podkomorzy zwycięzca wspaniałomyślny. \n",
      "(długość: 536)\n",
      "\n",
      "\n",
      "2. Plany myśliwskie Telimeny — Ogrodniczka wybiera się na wielki świat i słucha nauk opiekunki — Strzelcy wracają — Wielkie zadziwienie Tadeusza — Spotkanie się powtórne w Świątyni dumania i zgoda ułatwiona za pośrednictwem mrówek — U stołu wytacza się rzecz o łowach — Powieść Wojskiego o Rejtanie i księciu Denassów, przerwana — Zagajenie układów między stronami, także przerwane — Zjawisko z kluczem — Kłótnia — Hrabia z Gerwazym odbywają radę wojenną. \n",
      "(długość: 452)\n",
      "\n",
      "\n",
      "3. Zjawisko w papilotach budzi Tadeusza — Za późne postrzeżenie omyłki — Karczma — Emisariusz — Zręczne użycie tabakiery zwraca dyskusję na właściwą drogę — Matecznik — Niedźwiedź — Niebezpieczeństwo Tadeusza i Hrabiego — Trzy strzały — Spór Sagalasówki z Sanguszkówką rozstrzygniony na stronę jednorurki horeszkowskiej — Bigos — Wojskiego powieść o pojedynku Doweyki z Domeyką przerwana szczuciem kota — Koniec powieści o Doweyce i Domeyce. \n",
      "(długość: 438)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "longest_lines = pan_tadeusz_file.top(3, key=lambda line: len(line))\n",
    "\n",
    "print(\"Trzy najdłuższe linie w pliku:\")\n",
    "for i, line in enumerate(longest_lines, start=1):\n",
    "    print(f\"{i}. {line} \\n(długość: {len(line)})\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a7d8fe-0d42-4bc0-94b8-343008c01e30",
   "metadata": {},
   "source": [
    "# Zadanie 3\n",
    "Wykorzystując listę stopwords z adresu https://github.com/bieli/stopwords/blob/master/polish.stopwords.txt wykorzystaj akcje i transformacje RDD i wygeneruj listę unikalnych słów z pliku z treścią Pana Tadeusza (sprawdź transformację countByValue()) pomijając powyższe słowa stop oraz wszelkie znaki przestankowe. Wynik zapisz do słownika, a następnie do pliku json o nazwie pan_tadeusz_bag_of_words.json. Które słowo występuje w tym tekście najczęściej? Wyświetl je z wyników wygenerowanych powyżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bae8e78-4162-452a-ac8d-7b50598cb2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(sc.textFile(\"polish.stopwords.txt\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "042a94f3-f631-4db4-a40a-b0aa91ae58fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    extra_punctuation = \"—\"  # myślnik nie jest zawarty w punctuation a jest najczęstszym \"słowem\" w Panu Tadeuszu\n",
    "    all_punctuation = string.punctuation + extra_punctuation\n",
    "    return word.translate(str.maketrans('', '', all_punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "968059ec-e221-41b2-84e5-32d114b4dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_rdd = (\n",
    "    pan_tadeusz_file\n",
    "    .flatMap(lambda line: line.split())\n",
    "    .map(lambda word: remove_punctuation(word.lower()))\n",
    "    .filter(lambda word: word and word not in stopwords)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54187188-75ac-4046-abd6-b06c2f9eae89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "word_counts = words_rdd.countByValue()\n",
    "\n",
    "with open(\"pan_tadeusz_bag_of_words.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word_counts, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7584b8ab-c8b6-4088-afc8-18da0f6be9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Najczęściej występujące słowo: 'rzekł', liczba wystąpień: 155\n"
     ]
    }
   ],
   "source": [
    "most_common_word = max(word_counts.items(), key=lambda x: x[1])\n",
    "print(f\"Najczęściej występujące słowo: '{most_common_word[0]}', liczba wystąpień: {most_common_word[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
