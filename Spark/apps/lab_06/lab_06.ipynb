{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0623b3da-6289-43bb-93ca-f1fb940c3200",
   "metadata": {},
   "source": [
    "# Uruchomienie silnika Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "824b9ec5-d2d6-45de-bf47-9515eecc6ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_NAME'] = \"/opt/spark\"\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "# os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/spark/work-dir/.venv/bin/python3'\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/spark/work-dir/.venv/bin/python3'\n",
    "\n",
    "# można też spróbować wykorzystać moduł findspark do automatycznego odnalezienia miejsca instalacji sparka\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# lub\n",
    "# findspark.init(\"/opt/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b3c3b33-31f5-471f-8126-0db46c83a918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/20 14:20:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://f641dfb305eb:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Create-DataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Create-DataFrame>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder.master(\"spark://spark-master:7077\").appName(\"Create-DataFrame\").getOrCreate()\n",
    "# konfiguracja z określeniem liczby wątków (2) oraz ilości pamięci do wykorzystania poza stertą interpretera Pythona\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Create-DataFrame\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"4g\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd4fbcc-3361-4535-ac11-3b952d6f647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61e5b5f5-4ef5-47fc-bee5-1cd4f806eb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-20 14:20:32--  https://archive.ics.uci.edu/static/public/911/recipe+reviews+and+user+feedback+dataset.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘recipe+reviews+and+user+feedback+dataset.zip’\n",
      "\n",
      "recipe+reviews+and+     [          <=>       ]   2.02M   822KB/s    in 2.5s    \n",
      "\n",
      "2024-11-20 14:20:35 (822 KB/s) - ‘recipe+reviews+and+user+feedback+dataset.zip’ saved [2114088]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pobranie spakowanego zbioru za pomocą polecenia systemowego wget\n",
    "# strona datasetu: https://archive.ics.uci.edu/dataset/911/recipe+reviews+and+user+feedback+dataset\n",
    "!wget https://archive.ics.uci.edu/static/public/911/recipe+reviews+and+user+feedback+dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54997f29-47c2-4906-94ee-c01e14af6393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\t      recipe+reviews+and+user+feedback+dataset.zip\n",
      "lab_06.ipynb  recipe_reviews.zip\n"
     ]
    }
   ],
   "source": [
    "# listujemy zawartość bieżącego folderu\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "363c93d1-76f1-4f33-baee-1eb8b3b87add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zmiana nazwy pliku - nie jest konieczna, ale trzeba zmienić później ścieżkę w kolejnej komórce notatnika\n",
    "!mv recipe+reviews+and+user+feedback+dataset.zip recipe_reviews.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e66cf994-76fc-4582-91a2-53168d5cd0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wypakowujemy plik do podfolderu data\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"recipe_reviews.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b99141e2-6321-4080-b4e5-53c7f1f1dcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Recipe Reviews and User Feedback Dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "!ls ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d5663eb-de4d-4668-9525-146df46f79ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",recipe_number,recipe_code,recipe_name,comment_id,user_id,user_name,user_reputation,created_at,reply_count,thumbs_up,thumbs_down,stars,best_score,text\n",
      "0,001,14299,Creamy White Chili,sp_aUSaElGf_14299_c_2G3aneMRgRMZwXqIHmSdXSG1hEM,u_9iFLIhMa8QaG,Jeri326,1,1665619889,0,0,0,5,527,\"I tweaked it a little, removed onions because of onion haters in my house, used Italian seasoning instead of just oregano, and use a paprika/ cayenne mix and a little more than the recipe called for.. we like everything a bit more hot. The chili was amazing! It was easy to make and everyone absolutely loved it. It will now be a staple meal in our house.\"\n",
      "1,001,14299,Creamy White Chili,sp_aUSaElGf_14299_c_2FsPC83HtzCsQAtOxlbL6RcaPbY,u_Lu6p25tmE77j,Mark467,50,1665277687,0,7,0,5,724,\"Bush used to have a white chili bean and it made this recipe super simple. I’ve written to them and asked them to please!, bring them back\"\n"
     ]
    }
   ],
   "source": [
    "# sprawdzamy jak wyglądają 3 pierwsze linie pliku, widać, że pierwsza zawiera nagłówki kolumn a dane są oddzielone przecinkiem\n",
    "!head -3 \"data/Recipe Reviews and User Feedback Dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96778e71-af62-46da-9c76-d451819b6ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_reviews = spark.read.csv('./data/Recipe Reviews and User Feedback Dataset.csv', header=True, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf20d805-4528-46b7-b9e5-97f4937e5bf5",
   "metadata": {},
   "source": [
    "# Wyświetlanie danych i schematu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb73a39e-002e-44b4-94a1-686e9f181dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+------------------+--------------------+--------------+----------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "|_c0|recipe_number|recipe_code|       recipe_name|          comment_id|       user_id| user_name|user_reputation|created_at|reply_count|thumbs_up|thumbs_down|stars|best_score|                text|\n",
      "+---+-------------+-----------+------------------+--------------------+--------------+----------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "|  0|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_9iFLIhMa8QaG|   Jeri326|              1|1665619889|          0|        0|          0|    5|       527|I tweaked it a li...|\n",
      "|  1|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_Lu6p25tmE77j|   Mark467|             50|1665277687|          0|        7|          0|    5|       724|Bush used to have...|\n",
      "|  2|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_s0LwgpZ8Jsqq|Barbara566|             10|1664404557|          0|        3|          0|    5|       710|I have a very com...|\n",
      "|  3|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_fqrybAdYjgjG|jeansch123|              1|1661787808|          2|        2|          0|    0|       581|In your introduct...|\n",
      "|  4|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_XXWKwVhKZD69|  camper77|             10|1664913823|          1|        7|          0|    0|       820|Wonderful! I made...|\n",
      "+---+-------------+-----------+------------------+--------------------+--------------+----------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/20 14:20:43 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/lab_06/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# najpopularniejsza metoda ich pobrania to show(), ale jest ich więcej\n",
    "df_reviews.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49ac4531-1e45-4552-8395-992a84b7e979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- recipe_number: string (nullable = true)\n",
      " |-- recipe_code: string (nullable = true)\n",
      " |-- recipe_name: string (nullable = true)\n",
      " |-- comment_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- user_reputation: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- reply_count: string (nullable = true)\n",
      " |-- thumbs_up: string (nullable = true)\n",
      " |-- thumbs_down: string (nullable = true)\n",
      " |-- stars: string (nullable = true)\n",
      " |-- best_score: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rzut oka na schemę tego DataFrame\n",
    "df_reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3541cc48-7398-46bc-8e29-ca58e81e6320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# widać, że wszystkie kolumny są typu string, to jest domyślny sposób wczytywania danych przez spark z plain text\n",
    "# możemy jednak przekazać dodatkowy parametr, który na podstawie próbki danych spróbuje dobrać typ danych odpowiedni dla kolumny\n",
    "df_reviews = spark.read.csv('./data/Recipe Reviews and User Feedback Dataset.csv', header=True, sep=\",\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb037413-4717-4513-a7b3-fa950361956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- recipe_number: string (nullable = true)\n",
      " |-- recipe_code: string (nullable = true)\n",
      " |-- recipe_name: string (nullable = true)\n",
      " |-- comment_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- user_reputation: string (nullable = true)\n",
      " |-- created_at: integer (nullable = true)\n",
      " |-- reply_count: integer (nullable = true)\n",
      " |-- thumbs_up: integer (nullable = true)\n",
      " |-- thumbs_down: integer (nullable = true)\n",
      " |-- stars: integer (nullable = true)\n",
      " |-- best_score: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# po wypisaniu schemy widać zmianę\n",
    "df_reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c69128ac-3d07-4ef2-af40-36b8c58ca76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ramkę możemy również inicjalizować wskazując pożądane typy danych\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType, LongType\n",
    "\n",
    "data = [(\"James\",\"\",\"Smith\",36636,\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",40288,\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",42114,\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",39192,\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",1000)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\", StringType(), True), \\\n",
    "    StructField(\"user_id\", StringType(), True), \\\n",
    "    StructField(\"lastname\", StringType(), True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    # błąd konwersji \"\" na int!\n",
    "    # StructField(\"id\", LongType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", StringType(), True)\n",
    "    # chcielibyśmy tak, ale tutaj nie da się za bardzo - błąd konwersji int na decimal!\n",
    "    # StructField(\"salary\", DecimalType(10,2), True) \\\n",
    "  ])\n",
    "\n",
    "df_test = spark.createDataFrame(data=data,schema=schema)\n",
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91fbfd3c-9178-4dab-81bd-4266272a14ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: decimal(10,2) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# możemy wykonać rzutowanie po wczytaniu danych z większością kolumn typu tekstowego\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_test = df_test.withColumn(\"salary\", F.col(\"salary\").cast(\"decimal(10,2)\"))\n",
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "464e610e-b5ef-4994-8daf-4b821cd7f3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| salary|\n",
      "+-------+\n",
      "|3000.00|\n",
      "|4000.00|\n",
      "|4000.00|\n",
      "|4000.00|\n",
      "|1000.00|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_test.select(df_test.salary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4b19e86-9b74-4d38-82fc-2a4df2ff04a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18268"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ile wierszy w ramce?\n",
    "df_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f8e622a-1f94-4c0e-b18c-e869110db031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Column<'user_name'>, Column<'user_name'>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame składa się z obiektów typu Column dla każdej kolumny\n",
    "# API dla typu Column: https://spark.apache.org/docs/3.5.3/api/python/reference/pyspark.sql/api/pyspark.sql.Column.html\n",
    "\n",
    "# do kolumn możemy się odwoływać tak jak w pandas API, ale wynik jest inny\n",
    "df_reviews.user_name, df_reviews['user_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8adc8aa3-693e-4f10-ae63-a2cb57e3fcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| user_name|\n",
      "+----------+\n",
      "|   Jeri326|\n",
      "|   Mark467|\n",
      "|Barbara566|\n",
      "|jeansch123|\n",
      "|  camper77|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aby wyświetlić dane musimy wywoałać funkcję select na obiekcie dataframe\n",
    "\n",
    "df_reviews.select(df_reviews.user_name).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af097824-5a59-4921-acff-380d80d7e9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[user_name: string, user_reputation: string]\n",
      "+----------+---------------+\n",
      "| user_name|user_reputation|\n",
      "+----------+---------------+\n",
      "|   Jeri326|              1|\n",
      "|   Mark467|             50|\n",
      "|Barbara566|             10|\n",
      "|jeansch123|              1|\n",
      "|  camper77|             10|\n",
      "+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do funkcji select możemy przekazać wiele kolumn a wywołania podobnie jak dla RDD są leniwe\n",
    "print(df_reviews.select(df_reviews.user_name, df_reviews.user_reputation))\n",
    "# musimy więc wywołać funkcję, której wykonanie \"zmusi\" Sparka do wyliczenia jej wartości lub jawnie wywołać np. show\n",
    "df_reviews.select(df_reviews.user_name, df_reviews.user_reputation).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7251e080-8151-44e1-89ed-b49ef4dcda1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# można zmienić to domyślne zachowanie Spark, ale zazwyczaj nie jest to dobry pomysł, chyba, że zbiór jest mały\n",
    "# zmieniamy to poprzez edycję poniższego parametru\n",
    "# spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a649609e-0560-4a5a-b3ed-db064414968d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "| user_name|user_reputation|\n",
      "+----------+---------------+\n",
      "|   Jeri326|              1|\n",
      "|   Mark467|             50|\n",
      "|Barbara566|             10|\n",
      "|jeansch123|              1|\n",
      "|  camper77|             10|\n",
      "+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lub indeksując kolumny innym sposobem\n",
    "df_reviews.select(df_reviews['user_name'],df_reviews['user_reputation']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b6f603c-0ff9-4fbe-9558-1fbf82c1eb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/20 14:20:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/lab_06/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n",
      "[Stage 12:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|_c0|recipe_number|recipe_code|recipe_name|comment_id|user_id|user_name|user_reputation|created_at|reply_count|thumbs_up|thumbs_down|stars|best_score|text|\n",
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|  0|           45|         64|         74|        77|     80|       83|             84|        86|         86|       86|         86|   86|        86|  86|\n",
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "# policzymy teraz liczbę wartości NULL w każdej kolumnie\n",
    "df_reviews.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_reviews.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb5b4c-7fc2-4b57-8d4f-4e7cc2081611",
   "metadata": {},
   "source": [
    "# Filtrowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d245027a-ca60-4154-ad9e-013ff7cef9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|                 _c0|       recipe_number|recipe_code|recipe_name|comment_id|user_id|user_name|user_reputation|created_at|reply_count|thumbs_up|thumbs_down|stars|best_score|text|\n",
      "+--------------------+--------------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|      Thank you!!!!\"|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "| It was excellent!  |                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|The recipe was a ...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|A shoutout to the...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|This recipe is de...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "| like the time it...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|My one complaint ...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|Have never eaten ...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|I thinned the mix...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|When you write 1-...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|decided to make a...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|looked good.  I u...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|little sweeter th...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|I baked the crust...| just to crisp it...|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|I made it myself ...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|I will try adding...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|This recipe is a ...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "| big! I&#39;m wit...| WILL NOT MAKE it...|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|It says a small p...|                NULL|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "|                  So| I used the compl...|       NULL|       NULL|      NULL|   NULL|     NULL|           NULL|      NULL|       NULL|     NULL|       NULL| NULL|      NULL|NULL|\n",
      "+--------------------+--------------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/20 14:20:50 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/lab_06/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# rzućmy okiem na kilka wierszy gdzie w kolumnie recipe_name jest wartość NULL\n",
    "df_reviews.filter(df_reviews.recipe_code.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "568acd67-e464-42ad-83d9-16b35a947049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/20 14:20:50 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/lab_06/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18182"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zapisanie do nowej ramki danych bez wartości pustych\n",
    "df_reviews_clean = df_reviews.na.drop()\n",
    "df_reviews_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cabbbda8-6302-4d8e-9000-1abc35010083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/20 14:20:51 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/lab_06/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n",
      "[Stage 19:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|_c0|recipe_number|recipe_code|recipe_name|comment_id|user_id|user_name|user_reputation|created_at|reply_count|thumbs_up|thumbs_down|stars|best_score|text|\n",
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "|  0|            0|          0|          0|         0|      0|        0|              0|         0|          0|        0|          0|    0|         0|   0|\n",
      "+---+-------------+-----------+-----------+----------+-------+---------+---------------+----------+-----------+---------+-----------+-----+----------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# dla pewności możemy to sprawdzić raz jeszcze\n",
    "df_reviews_clean.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_reviews_clean.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7849f432-b1b3-4fb2-96e5-d3708a8c32aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/20 14:20:52 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/lab_06/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|     user_name|\n",
      "+--------------+\n",
      "|         ahmom|\n",
      "|  annamossburg|\n",
      "|   astarzynski|\n",
      "|     adamscook|\n",
      "|      angela32|\n",
      "|       annaf27|\n",
      "|    avanhaasen|\n",
      "|    aunt ann's|\n",
      "|     angelic0w|\n",
      "|a_n_g_e_l_0715|\n",
      "+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---+-------------+-----------+------------------+--------------------+--------------+----------------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "|_c0|recipe_number|recipe_code|       recipe_name|          comment_id|       user_id|       user_name|user_reputation|created_at|reply_count|thumbs_up|thumbs_down|stars|best_score|                text|\n",
      "+---+-------------+-----------+------------------+--------------------+--------------+----------------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "|  0|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_9iFLIhMa8QaG|         Jeri326|              1|1665619889|          0|        0|          0|    5|       527|I tweaked it a li...|\n",
      "|  1|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_Lu6p25tmE77j|         Mark467|             50|1665277687|          0|        7|          0|    5|       724|Bush used to have...|\n",
      "|  2|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_s0LwgpZ8Jsqq|      Barbara566|             10|1664404557|          0|        3|          0|    5|       710|I have a very com...|\n",
      "|  5|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_BALTQJIvWtYr|         nikhita|              1|1661354351|          0|        3|          1|    5|       518|amazing! my boyfr...|\n",
      "|  6|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_HuJVXMzQqJoI|       Sandy1256|              1|1644088805|          0|       11|          0|    5|       833|Wow!!!  This reci...|\n",
      "|  8|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_xDTU4BqIVIc9|           Quest|              1|1643933124|          0|        6|          0|    5|       693|I absolutely love...|\n",
      "|  9|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_cDoX9ujcQEoc|     Susannah953|              1|1643237839|          0|        0|          0|    5|       404|I make this a lot...|\n",
      "| 10|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_a1kMC63ejmcn|jillanglemyer810|              1|1643127683|          0|        3|          0|    5|       706|Best and easiest ...|\n",
      "| 11|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_rLDRFmNx35zU|       Leslie126|              1|1642892566|          0|        2|          1|    5|       617|Best white chili ...|\n",
      "| 12|          001|      14299|Creamy White Chili|sp_aUSaElGf_14299...|u_lPW6uyGJNSN0|       Cindy4876|              1|1642353692|          0|        1|          0|    5|       633|This recipe was e...|\n",
      "+---+-------------+-----------+------------------+--------------------+--------------+----------------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/20 14:20:52 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/lab_06/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# filtrowanie danych z ramki\n",
    "df_reviews_clean.filter(df_reviews.user_name.startswith('a')).select(df_reviews_clean.user_name).show(10)\n",
    "df_reviews_clean.filter(df_reviews.stars == 5).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcfffc64-11e5-428c-90b7-7e263d7b7e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/20 14:20:52 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/lab_06/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|    avg(thumbs_up)|\n",
      "+------------------+\n",
      "|1.0892641073589264|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# wyliczenie średniej wartości z kolumny\n",
    "df_reviews_clean.select(avg(df_reviews_clean.thumbs_up)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ff30eaf-65ce-4267-b244-229d3b02f757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/20 14:20:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/lab_06/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         thumbs_up|\n",
      "+-------+------------------+\n",
      "|  count|             18182|\n",
      "|   mean|1.0892641073589264|\n",
      "| stddev| 4.201003572820717|\n",
      "|    min|                 0|\n",
      "|    max|               106|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ale możemy się dowiedzieć tego i więcej w sposób podobny do tego z biblioteki pandas\n",
    "df_reviews_clean.select(df_reviews_clean.thumbs_up).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c90dc76f-6542-44f6-8c4b-cad2d3936732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/20 14:20:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/lab_06/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|recipe_code|sum(thumbs_down)|\n",
      "+-----------+----------------+\n",
      "|       2832|             488|\n",
      "|       9739|             354|\n",
      "|      17826|             328|\n",
      "|      18345|             313|\n",
      "|      12003|             306|\n",
      "|       4383|             301|\n",
      "|      41095|             275|\n",
      "|       8202|             272|\n",
      "|       6504|             264|\n",
      "|       6086|             262|\n",
      "+-----------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "df_reviews_clean.groupby('recipe_code').agg({'thumbs_down': 'sum'}).sort(desc('sum(thumbs_down)')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef4d5b06-af71-4a24-89e5-682d0d5004bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deklaracja zbiorów wartości dla poszczególnych kolumn przyszłego zbioru danych\n",
    "header = ['id', 'firstname', 'lastname', 'age', 'salary']\n",
    "firstnames = ['Adam', 'Katarzyna', 'Krzysztof', 'Marek', 'Aleksandra', 'Zbigniew', 'Wojciech', 'Mieczysław', 'Agata', 'Wisława']\n",
    "lastnames = ['Mieczykowski', 'Kowalski', 'Malinowski' , 'Szczaw', 'Glut', 'Barański', 'Brzęczyszczykiewicz', 'Wróblewski', 'Wlotka', 'Pysla']\n",
    "age = {'min': 18, 'max': 68}\n",
    "salary = {'min': 3200, 'max': 12500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "713a2380-1a1d-420f-89dc-2626d5bacf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja do generowania fikcyjnego datasetu\n",
    "# n_rows oznacza ilość wierszy, którą chcemy finalnie uzyskać\n",
    "\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_dataset(filename, n_rows=100, chunk_size=100000):\n",
    "    rows = []\n",
    "    rows.append(header)\n",
    "    mu = (salary['max'] + salary['min']) / 2\n",
    "    sigma = 1000\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as filehandler:\n",
    "        \n",
    "        for id in tqdm(range(1, n_rows + 1), total=n_rows, desc=\"Building dataset...\"):\n",
    "            row = [\n",
    "                f'{id}', \n",
    "                f'{random.choice(firstnames)}', \n",
    "                f'{random.choice(lastnames)}', \n",
    "                f\"{random.randint(age['min'], age['max'])}\",\n",
    "                f\"{round(float(random.normalvariate(mu=mu, sigma=sigma)), 2)}\"\n",
    "            ]\n",
    "            rows.append(row)\n",
    "            if id % chunk_size == 0:\n",
    "                filehandler.writelines([f\"{','.join(row)}\\n\" for row in rows])\n",
    "                rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "183b8537-5c8d-4000-9ba3-7b1d343e8735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building dataset...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000000/2000000 [00:13<00:00, 146965.31it/s]\n"
     ]
    }
   ],
   "source": [
    "build_dataset('employee.csv', 2_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6130e7a-52c2-466c-9930-a5897268330e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.36 ms, sys: 0 ns, total: 7.36 ms\n",
      "Wall time: 3.26 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# więcej magicznych metod w Jupyter Notebooku: https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "# wczytanie pliku csv przez spark\n",
    "# df = spark.read.csv('employee.csv', header=True)\n",
    "df = spark.read.csv('employee.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9155cf1b-91c9-4085-a5cd-ddbe642cd21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fa2539a-8ddd-4b23-8065-4d6d08ac5126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "+---+----------+------------+---+-------+\n",
      "| id| firstname|    lastname|age| salary|\n",
      "+---+----------+------------+---+-------+\n",
      "|  1|  Wojciech|      Szczaw| 62|8132.22|\n",
      "|  2|  Zbigniew|      Wlotka| 66|8268.47|\n",
      "|  3|     Agata|Mieczykowski| 37|6116.28|\n",
      "|  4|  Zbigniew|  Wróblewski| 25|9377.25|\n",
      "|  5|      Adam|    Kowalski| 30|6995.95|\n",
      "|  6|     Marek|      Wlotka| 60|8800.12|\n",
      "|  7|  Wojciech|    Kowalski| 49| 8335.9|\n",
      "|  8|  Zbigniew|       Pysla| 37|5548.33|\n",
      "|  9| Katarzyna|Mieczykowski| 58| 7164.0|\n",
      "| 10|Aleksandra|    Barański| 68|7037.87|\n",
      "+---+----------+------------+---+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 0 ns, sys: 7.48 ms, total: 7.48 ms\n",
      "Wall time: 205 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# wypisujemy schemat i 10 pierwszych wierszy utworzonego obiektu Spark DataFrame\n",
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27e00d57-fffa-47b6-9462-103b5bd2f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# przykład wykorzystania funkcji transform, która mapuje wykonanie stworzonej funkcji tu_upper_str_columns na istniejącą kolumnę\n",
    "# i zwraca nową ramkę z dodatkową kolumną\n",
    "from pyspark.sql.functions import upper\n",
    "\n",
    "def to_upper_str_columns(df, column_name, new_column_name):\n",
    "    return df.withColumn(new_column_name, upper(df[column_name]))\n",
    "\n",
    "df = df.transform(to_upper_str_columns, \"firstname\", \"firstname_upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6916f04a-ba4d-4175-b00d-a814bac0ae28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+---+-------+---------------+\n",
      "| id| firstname|    lastname|age| salary|firstname_upper|\n",
      "+---+----------+------------+---+-------+---------------+\n",
      "|  1|  Wojciech|      Szczaw| 62|8132.22|       WOJCIECH|\n",
      "|  2|  Zbigniew|      Wlotka| 66|8268.47|       ZBIGNIEW|\n",
      "|  3|     Agata|Mieczykowski| 37|6116.28|          AGATA|\n",
      "|  4|  Zbigniew|  Wróblewski| 25|9377.25|       ZBIGNIEW|\n",
      "|  5|      Adam|    Kowalski| 30|6995.95|           ADAM|\n",
      "|  6|     Marek|      Wlotka| 60|8800.12|          MAREK|\n",
      "|  7|  Wojciech|    Kowalski| 49| 8335.9|       WOJCIECH|\n",
      "|  8|  Zbigniew|       Pysla| 37|5548.33|       ZBIGNIEW|\n",
      "|  9| Katarzyna|Mieczykowski| 58| 7164.0|      KATARZYNA|\n",
      "| 10|Aleksandra|    Barański| 68|7037.87|     ALEKSANDRA|\n",
      "+---+----------+------------+---+-------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f2d88c0-ae59-462f-aab7-b92b47df4dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "31464"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtrowanie numeryczne, ale tu na kolumnie typu str - czy jest poprawne? - nie\n",
    "df.filter(df[\"salary\"] > 10000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc8c8e6d-ebe3-454e-886e-7080377293f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# na ile partycji została nasza ramka danych rozrzucona po \"klastrze\"?\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "969225e2-c9ef-4fc9-8469-06d2906c7d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.15 ms, sys: 0 ns, total: 7.15 ms\n",
      "Wall time: 2.41 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "31464"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# mierzymy czas operacji przy domyślnej liczbie partycji\n",
    "df.filter(df[\"salary\"] > 10000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "014048e3-b194-4ee7-8d47-f865db78203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a897dff-0e0e-4a05-b5e1-bb731cdf044d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d818bccf-2c01-462b-a068-b660e5141b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.78 ms, sys: 648 μs, total: 6.43 ms\n",
      "Wall time: 510 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "31464"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# mierzymy czas operacji przy 6 partycjach dla 2_000_000 rekordów\n",
    "df.filter(df[\"salary\"] > 10000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43a3e76d-c81c-4c6e-ae69-1a11d73acb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                24/11/20 14:26:07 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 65:======================================>                   (4 + 2) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|firstname_age|  18|  19|  20|  21|  22|  23|  24|  25|  26|  27|  28|  29|  30|  31|  32|  33|  34|  35|  36|  37|  38|  39|  40|  41|  42|  43|  44|  45|  46|  47|  48|  49|  50|  51|  52|  53|  54|  55|  56|  57|  58|  59|  60|  61|  62|  63|  64|  65|  66|  67|  68|\n",
      "+-------------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|         Adam|3902|3916|3953|3914|3903|3911|3976|3860|3870|3885|3893|3896|3992|3962|3952|4025|3879|3900|3898|3875|3936|3971|3870|3968|4006|3965|3903|3892|3856|3969|3839|4062|3908|3957|3988|3850|3831|3937|3955|3838|3801|3905|3969|3914|3865|3924|3777|3890|3892|3938|4004|\n",
      "|        Agata|3896|3959|3920|3935|3841|3976|3918|3814|3954|3991|3847|3819|3979|3921|3969|3897|3838|4030|3841|3949|3936|3838|3853|3951|3901|3993|3958|3863|3807|3926|3987|3882|3931|3770|3776|3902|3981|3998|3908|3980|3971|3849|3966|3999|3870|4012|3950|3930|4032|3965|3810|\n",
      "|   Aleksandra|3951|3964|3856|3829|3822|3936|3867|3965|4015|3897|3874|3951|4009|3901|4038|3911|3920|3984|3872|4069|3878|3881|3911|3943|3901|3961|3838|3916|3948|3892|3909|3942|4013|4018|3917|3912|3825|3918|3863|3894|3928|3996|3894|4015|3993|3950|3943|3931|3932|3982|3830|\n",
      "|    Katarzyna|3874|3856|3884|3985|3922|3897|3931|3887|3975|3944|3899|3850|3854|3899|3955|3900|3811|3944|3890|3822|3931|3966|3805|3868|3986|3999|4000|4026|3835|3990|4076|3888|3832|3968|3868|3940|3908|4016|3868|4009|3873|3908|3937|3967|4049|3773|3850|3900|3929|3986|3892|\n",
      "|    Krzysztof|3924|3988|3874|3919|3973|3933|3874|4006|3926|3997|3921|4051|3831|3859|3925|3910|3908|4014|3988|3919|3934|4028|3849|3935|4062|3778|3861|3857|3862|4052|3922|3888|3908|3850|4004|3957|3929|3976|3926|3909|3978|3905|3950|3996|4017|4027|3929|3917|3859|3851|3996|\n",
      "|        Marek|3905|3916|3971|3820|3875|3920|3845|3933|3888|3920|3892|3900|3963|3805|3898|4032|3911|3909|3899|3971|4047|3860|3813|3966|3912|3972|3814|3949|3851|3961|3873|3815|3889|3940|3922|3949|3913|3888|3886|3969|3831|3919|3945|3894|3911|3980|3837|3935|3820|3889|3875|\n",
      "|   Mieczysław|3995|3890|3928|4022|3845|3837|3930|3955|3934|3897|3992|3898|3862|3921|3903|3878|3943|3792|3968|3901|3910|3916|3913|3956|3901|3886|3991|3909|3883|3946|3932|3907|3953|3862|3918|3927|3991|3880|3814|3991|3943|3949|3929|3984|3890|4087|3977|3978|3868|3798|3900|\n",
      "|      Wisława|3850|3858|3909|4059|3977|3846|3886|3947|3854|3810|3911|4048|3929|3866|3926|3757|3952|3887|3865|3974|3883|3950|4022|3941|3927|3820|3925|3948|3974|3946|3967|3946|3830|3870|4058|3817|3950|3936|3937|3995|3963|3909|3940|3900|4043|3933|3879|4019|4066|3894|3938|\n",
      "|     Wojciech|3921|3889|3844|3910|4053|3918|3910|3799|3862|3865|4046|3971|3944|3776|3917|3973|3954|3869|3896|3946|3821|3923|3996|3937|3873|3935|3885|3924|3992|3774|3970|3860|3952|3909|3986|3946|3937|3737|3979|3920|3856|4023|3940|3852|3817|4063|3877|3839|4005|4018|3921|\n",
      "|     Zbigniew|4030|3917|4012|3860|3833|3894|3827|3996|3897|3915|3944|4003|3955|3933|3846|3887|3937|3939|4025|3937|4035|3967|4010|4016|4015|3918|3814|3878|3937|4038|3916|3957|3841|3814|3807|3914|4023|4003|3899|3851|3980|3929|3878|3847|3967|3918|3940|3954|4009|3916|3967|\n",
      "+-------------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# macierz częstości dla dwóch kolumn - uwaga dla bardzo różnorodnych danych!\n",
    "df.crosstab(\"firstname\", \"age\").sort(\"firstname_age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffa0d044-a666-4e3d-bd9a-8ddb61b3af84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (5)\n",
      "+- Exchange (4)\n",
      "   +- Project (3)\n",
      "      +- Filter (2)\n",
      "         +- Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [5]: [id#1055, firstname#1056, lastname#1057, age#1058, salary#1059]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/opt/spark/work-dir/lab_06/employee.csv]\n",
      "PushedFilters: [IsNotNull(firstname), StringContains(firstname,ski)]\n",
      "ReadSchema: struct<id:int,firstname:string,lastname:string,age:int,salary:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [5]: [id#1055, firstname#1056, lastname#1057, age#1058, salary#1059]\n",
      "Condition : (isnotnull(firstname#1056) AND Contains(firstname#1056, ski))\n",
      "\n",
      "(3) Project\n",
      "Output [6]: [id#1055, firstname#1056, lastname#1057, age#1058, salary#1059, upper(firstname#1056) AS firstname_upper#1092]\n",
      "Input [5]: [id#1055, firstname#1056, lastname#1057, age#1058, salary#1059]\n",
      "\n",
      "(4) Exchange\n",
      "Input [6]: [id#1055, firstname#1056, lastname#1057, age#1058, salary#1059, firstname_upper#1092]\n",
      "Arguments: RoundRobinPartitioning(6), REPARTITION_BY_NUM, [plan_id=1068]\n",
      "\n",
      "(5) AdaptiveSparkPlan\n",
      "Output [6]: [id#1055, firstname#1056, lastname#1057, age#1058, salary#1059, firstname_upper#1092]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# funkcja explain może przydać się w przypadku bardziej zaawansowanego debuggingu, optymalizacji i zrozumienia\n",
    "# kolejności działania niektórych elementów silnika Spark\n",
    "query = df.filter(df.firstname.contains('ski'))\n",
    "query.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f37040d1-6360-42f3-9d03-814f4ac3dfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# zapisujemy ramkę do plików parquet\n",
    "# zwróć uwagę na liczbę utworzonych plików\n",
    "\n",
    "df.write.parquet('./data/parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b19af7-4ec4-4bfd-b909-4d1810e86097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lub chcąc nadpisać już istniejące dane - w trybie overwrite\n",
    "# df.write.mode(\"overwrite\").parquet('./data/parquet/')\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788cf45a-c2dd-4dea-b5c4-c1bed7540bfc",
   "metadata": {},
   "source": [
    "# Zadanie 1\n",
    "Na zbiorze danych 'Recipe Reviews ...' wykonaj:\n",
    "* 1.1 Zmień nazwę pierwszej kolumny z _c0 na id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "acd649d7-3820-41c0-9ae4-d2dcc73fcdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- recipe_number: string (nullable = true)\n",
      " |-- recipe_code: string (nullable = true)\n",
      " |-- recipe_name: string (nullable = true)\n",
      " |-- comment_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- user_reputation: string (nullable = true)\n",
      " |-- created_at: integer (nullable = true)\n",
      " |-- reply_count: integer (nullable = true)\n",
      " |-- thumbs_up: integer (nullable = true)\n",
      " |-- thumbs_down: integer (nullable = true)\n",
      " |-- stars: integer (nullable = true)\n",
      " |-- best_score: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_reviews = df_reviews.withColumnRenamed(\"_c0\", \"id\")\n",
    "df_reviews.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7193cebf-a522-40a6-b004-caa0ccf7f6f5",
   "metadata": {},
   "source": [
    "* 1.2 Wyświetl 10 najwyższych wartości w kolumnie reply_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68a7829a-378d-4af9-a63d-dac67d54952f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/20 14:40:37 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      " Schema: _c0, recipe_number, recipe_code, recipe_name, comment_id, user_id, user_name, user_reputation, created_at, reply_count, thumbs_up, thumbs_down, stars, best_score, text\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///opt/spark/work-dir/lab_06/data/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+--------------------+--------------------+--------------------+-----------------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "| id|recipe_number|recipe_code|         recipe_name|          comment_id|             user_id|        user_name|user_reputation|created_at|reply_count|thumbs_up|thumbs_down|stars|best_score|                text|\n",
      "+---+-------------+-----------+--------------------+--------------------+--------------------+-----------------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "|  8|          003|       2832|   Cheeseburger Soup|sp_aUSaElGf_2832_...|      u_inTFTX8AEJ0X|   ladypenny36619|              1|1659170678|          3|        1|         20|    5|       495|I love this recip...|\n",
      "| 17|          002|       3309|Best Ever Banana ...|sp_aUSaElGf_3309_...|u_1tOHujEFJQIEVu0...|     OrangeBowtie|              0|1622648873|          3|        5|          0|    5|       354|The title is not ...|\n",
      "| 40|          003|       2832|   Cheeseburger Soup|sp_aUSaElGf_2832_...|u_1oKd0xoTyJRTE0q...|                K|              1|1622648880|          3|        6|         41|    0|       127|Just once I&#39;d...|\n",
      "| 43|          003|       2832|   Cheeseburger Soup|sp_aUSaElGf_2832_...|u_1oKd0qIiH6zumOD...|             Elle|              1|1622648880|          3|        4|         61|    2|       110|I made this recip...|\n",
      "|  5|          013|      32480|Basic Homemade Bread|sp_aUSaElGf_32480...|      u_mliuhxCdSTIo|         James626|              1|1655857389|          3|       15|         31|    0|       549|I waited three ho...|\n",
      "|  0|          040|       8431|Rhubarb Custard Bars|sp_aUSaElGf_8431_...|      u_VgdSEmx4XglO|       Frances442|              1|1656521871|          3|        0|          0|    0|       505|Hello, we usually...|\n",
      "| 27|          083|       9735|Comforting Chicke...|sp_aUSaElGf_9735_...|u_1oKbxCQrwYGMt3E...|Beverly Kowulich |              1|1622718406|          2|        1|          1|    4|       142|That’s a lot of w...|\n",
      "|  3|          001|      14299|  Creamy White Chili|sp_aUSaElGf_14299...|      u_fqrybAdYjgjG|       jeansch123|              1|1661787808|          2|        2|          0|    0|       581|In your introduct...|\n",
      "|  7|          081|      27626|Skillet Shepherd’...|sp_aUSaElGf_27626...|      u_faZ6tUcsyiHE|          Dale732|              1|1632118219|          2|       13|        104|    0|       335|I am always amaze...|\n",
      "| 25|          006|      21444|Favorite Chicken ...|sp_aUSaElGf_21444...|u_1tOHukW0cVBCqkC...|          BlueBee|              0|1622648873|          2|       10|         31|    0|       162|A cup of butter a...|\n",
      "+---+-------------+-----------+--------------------+--------------------+--------------------+-----------------+---------------+----------+-----------+---------+-----------+-----+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_reviews.orderBy(desc(\"reply_count\")).limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babbf8b4-21c3-46e6-af40-1d9a64773b5d",
   "metadata": {},
   "source": [
    "* 1.3 Wyświetl 10 najwyższych sum wartości w kolumnie best_score dla każdego przepisu (grupowanie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "467a29c3-2373-4bde-9c23-147bef1bb69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|         recipe_name|total_best_score|\n",
      "+--------------------+----------------+\n",
      "|   Cheeseburger Soup|           98863|\n",
      "|  Creamy White Chili|           85497|\n",
      "|Amish Breakfast C...|           64880|\n",
      "|Best Ever Banana ...|           64247|\n",
      "|Favorite Chicken ...|           60755|\n",
      "|Basic Homemade Bread|           59867|\n",
      "|Flavorful Chicken...|           59195|\n",
      "|Zucchini Pizza Ca...|           54032|\n",
      "|Enchilada Casser-...|           51975|\n",
      "|    Cauliflower Soup|           47905|\n",
      "+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, sum, desc\n",
    "\n",
    "df_reviews = df_reviews.withColumn(\n",
    "    \"best_score\", \n",
    "    regexp_replace(col(\"best_score\"), \"[^0-9]\", \"\").cast(\"int\")\n",
    ")\n",
    "\n",
    "grouped_scores = (\n",
    "    df_reviews.groupBy(\"recipe_name\")\n",
    "    .agg(sum(\"best_score\").alias(\"total_best_score\"))\n",
    "    .orderBy(desc(\"total_best_score\"))\n",
    ")\n",
    "\n",
    "grouped_scores.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0034c1d-9db3-4a08-9c2c-1041b711447a",
   "metadata": {},
   "source": [
    "* 1.4 Które 10 przepisów miało najwięcej komentarzy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f72bda97-995e-41d7-b977-12dea0515cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|         recipe_name|total_comments|\n",
      "+--------------------+--------------+\n",
      "|   Cheeseburger Soup|            16|\n",
      "|Lemon Blueberry B...|            13|\n",
      "|Basic Homemade Bread|            12|\n",
      "|Simple Au Gratin ...|            10|\n",
      "|  Creamy White Chili|            10|\n",
      "|    Simple Taco Soup|             9|\n",
      "| Traditional Lasagna|             8|\n",
      "|Contest-Winning N...|             8|\n",
      "| Porcupine Meatballs|             8|\n",
      "|Favorite Chicken ...|             6|\n",
      "+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_comments = (\n",
    "    df_reviews.groupBy(\"recipe_name\")\n",
    "    .agg(sum(\"reply_count\").alias(\"total_comments\"))\n",
    "    .orderBy(desc(\"total_comments\"))\n",
    ")\n",
    "\n",
    "grouped_comments.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a425c1d4-542a-474f-8704-fe6672a15088",
   "metadata": {},
   "source": [
    "* 1.5 Wyświetl rozkład wartości w kolumnie stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb72f151-8060-449b-80a0-f54e91934eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|stars|count|\n",
      "+-----+-----+\n",
      "|    5|13829|\n",
      "|    0| 1696|\n",
      "|    4| 1655|\n",
      "|    3|  490|\n",
      "|    1|  280|\n",
      "|    2|  232|\n",
      "| NULL|   86|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stars_distribution = (\n",
    "    df_reviews.groupBy(\"stars\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    ")\n",
    "\n",
    "stars_distribution.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e015ed-154d-4a53-8f05-7cd82fa66906",
   "metadata": {},
   "source": [
    "# Zadanie 2\n",
    "Wczytaj zbiór danych employee nakazując Sparkowi wywnioskowanie bardziej optymalnych typów danych niż domyślny typ string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fbd7c4d4-5a16-469e-b59d-a2237606934b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "+---+---------+------------+---+-------+\n",
      "| id|firstname|    lastname|age| salary|\n",
      "+---+---------+------------+---+-------+\n",
      "|  1| Wojciech|      Szczaw| 62|8132.22|\n",
      "|  2| Zbigniew|      Wlotka| 66|8268.47|\n",
      "|  3|    Agata|Mieczykowski| 37|6116.28|\n",
      "|  4| Zbigniew|  Wróblewski| 25|9377.25|\n",
      "|  5|     Adam|    Kowalski| 30|6995.95|\n",
      "+---+---------+------------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_employee = spark.read.csv(\n",
    "    \"employee.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df_employee.printSchema()\n",
    "\n",
    "df_employee.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c406e1-b64d-45c5-9634-4e9605a7b731",
   "metadata": {},
   "source": [
    "# Zadanie 3\n",
    "Jaki jest czas wykonania operacji df.filter(df[\"salary\"] > 10000).count() tym razem przy numerycznym typie kolumny salary? Jest jakaś różnica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6b2cb25-9f63-4a72-b810-edd80dc62cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.21 ms, sys: 934 μs, total: 7.15 ms\n",
      "Wall time: 216 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "31464"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df.filter(df[\"salary\"] > 10000).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaeb99b-2bce-4e7a-b727-e47c0f754b73",
   "metadata": {},
   "source": [
    "Jest szybciej o 300ms / 2 sekundy w zależności od liczby partycji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae595df5-c437-464e-9614-2604fb35dd93",
   "metadata": {},
   "source": [
    "# Zadanie 4\n",
    "Wykorzystując przykład z dokumentacji klasy Bucketizer (https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Bucketizer.html) podziel dane w kolumnie age zbioru employee na buckety co 10 lat (10-19, 20-29, ..., 60-69) i wyświetl te dane dla 20 pierwszych wierzy w formie surowej oraz całość grupując po bucketach i licząc ile osób znalazło się w każdym z nich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2f3f74a4-e983-4c5c-a1dc-bcaae5d91f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|age|age_range|\n",
      "+---+---------+\n",
      "| 62|    60-69|\n",
      "| 66|    60-69|\n",
      "| 37|    30-39|\n",
      "| 25|    20-29|\n",
      "| 30|    30-39|\n",
      "| 60|    60-69|\n",
      "| 49|    40-49|\n",
      "| 37|    30-39|\n",
      "| 58|    50-59|\n",
      "| 68|    60-69|\n",
      "| 61|    60-69|\n",
      "| 62|    60-69|\n",
      "| 54|    50-59|\n",
      "| 20|    20-29|\n",
      "| 62|    60-69|\n",
      "| 47|    40-49|\n",
      "| 45|    40-49|\n",
      "| 65|    60-69|\n",
      "| 18|    10-19|\n",
      "| 55|    50-59|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [10, 20, 30, 40, 50, 60, 70]\n",
    "\n",
    "bucketizer = Bucketizer(\n",
    "    splits=splits,\n",
    "    inputCol=\"age\",\n",
    "    outputCol=\"age_bucket\"\n",
    ")\n",
    "\n",
    "bucketed_df = bucketizer.transform(df_employee)\n",
    "\n",
    "bucketed_df = bucketed_df.withColumn(\n",
    "    \"age_range\",\n",
    "    when(col(\"age_bucket\") == 0.0, \"10-19\")\n",
    "    .when(col(\"age_bucket\") == 1.0, \"20-29\")\n",
    "    .when(col(\"age_bucket\") == 2.0, \"30-39\")\n",
    "    .when(col(\"age_bucket\") == 3.0, \"40-49\")\n",
    "    .when(col(\"age_bucket\") == 4.0, \"50-59\")\n",
    "    .when(col(\"age_bucket\") == 5.0, \"60-69\")\n",
    "    .otherwise(\"Unknown\")\n",
    ")\n",
    "\n",
    "\n",
    "bucketed_df.select(\"age\", \"age_range\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bfd3f66e-3cd6-4d12-acad-623a7a10a275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 108:>                                                        (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|age_range| count|\n",
      "+---------+------+\n",
      "|    10-19| 78401|\n",
      "|    20-29|391544|\n",
      "|    30-39|392097|\n",
      "|    40-49|392362|\n",
      "|    50-59|391757|\n",
      "|    60-69|353839|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "grouped_buckets = (\n",
    "    bucketed_df.groupBy(\"age_range\")\n",
    "    .count()\n",
    "    .orderBy(\"age_range\")\n",
    ")\n",
    "\n",
    "grouped_buckets.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
