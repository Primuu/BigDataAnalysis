{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3b5ca8-1422-486c-9ccb-5e571c497df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0132168b-f909-45d3-b23f-ad9a0d935d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/21 19:17:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://f641dfb305eb:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Apache SQL and Hive</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Apache SQL and Hive>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ścieżka do bazy danych hurtowni danych oraz plików\n",
    "warehouse_location = '/opt/spark/work-dir/lab_07/metastore_db'\n",
    "\n",
    "# utworzenie sesji Spark, ze wskazaniem włączenia obsługi Hive oraz\n",
    "# lokalizacją przechowywania hurtowni danych\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Apache SQL and Hive\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"4g\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse_location)\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19e2ae3-d76b-4547-b6df-3939f445d648",
   "metadata": {},
   "source": [
    "# 1. Spark i SQL\n",
    "Spark umożliwia zarejestrowanie obiektu DataFrame jako widoku, co umożliwia korzystanie z niego w sposób bardzo zbliżony do pracy z językiem SQL. Poniżej przykład."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "366c3945-66e9-466d-9c22-ab8cc8b02e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\n",
    "    '../lab_06/employee.csv', \n",
    "    header=True, \n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cbf99a9-f0d6-45a1-82da-0739e8e802c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy widok tymczasowy w pamięci węzła\n",
    "df.createOrReplaceTempView(\"EMPLOYEE_DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8819121c-5e13-4da2-9443-c955cb810433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 19:18:48 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/11/21 19:18:48 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/11/21 19:19:08 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/11/21 19:19:08 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.2\n",
      "24/11/21 19:19:08 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='EMPLOYEE_DATA', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wypisanie tabeli, zwróć uwagę na to, czy stworzona tabela jest tymczasowa czy trwała\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c0bc076-4e07-4fd0-817f-a91fb9d38aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------------+---+-------+\n",
      "| id|firstname|    lastname|age| salary|\n",
      "+---+---------+------------+---+-------+\n",
      "|  1| Wojciech|      Szczaw| 62|8132.22|\n",
      "|  2| Zbigniew|      Wlotka| 66|8268.47|\n",
      "|  3|    Agata|Mieczykowski| 37|6116.28|\n",
      "|  4| Zbigniew|  Wróblewski| 25|9377.25|\n",
      "+---+---------+------------+---+-------+\n",
      "\n",
      "+----------+\n",
      "| firstname|\n",
      "+----------+\n",
      "|  Wojciech|\n",
      "|  Zbigniew|\n",
      "|     Agata|\n",
      "|  Zbigniew|\n",
      "|      Adam|\n",
      "|     Marek|\n",
      "|  Wojciech|\n",
      "|  Zbigniew|\n",
      "| Katarzyna|\n",
      "|Aleksandra|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pobranie danych jak z tabeli SQL\n",
    "spark.sql(\"Select * from EMPLOYEE_DATA limit 4\").show()\n",
    "spark.sql(\"select firstname from EMPLOYEE_DATA\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b667f9cf-481d-4968-9606-f8dfd2163449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=============================>                             (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+------------------+\n",
      "| firstname|count(firstname)|       avg(salary)|\n",
      "+----------+----------------+------------------+\n",
      "|   Wisława|          200237| 7851.794412820825|\n",
      "|Mieczysław|          199980| 7848.768358535832|\n",
      "|     Agata|          199789| 7849.547957244942|\n",
      "| Krzysztof|          200652| 7850.057565087836|\n",
      "|     Marek|          199198|7847.2436489824695|\n",
      "|      Adam|          199742| 7851.878832694189|\n",
      "| Katarzyna|          199822| 7851.342516789995|\n",
      "|  Wojciech|          199730| 7849.594330596333|\n",
      "|  Zbigniew|          200545| 7849.557810566169|\n",
      "|Aleksandra|          200305| 7849.418310676244|\n",
      "+----------+----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark.sql(\"select firstname, count(firstname), avg(salary) from EMPLOYEE_DATA group by firstname\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfaa407c-db20-4c3b-b194-1ef91abecd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-------+------------+\n",
      "|firstname|    lastname| salary|after_rising|\n",
      "+---------+------------+-------+------------+\n",
      "| Wojciech|      Szczaw|8132.22|     8945.44|\n",
      "| Zbigniew|      Wlotka|8268.47|     9095.32|\n",
      "|    Agata|Mieczykowski|6116.28|     6727.91|\n",
      "| Zbigniew|  Wróblewski|9377.25|    10314.98|\n",
      "|     Adam|    Kowalski|6995.95|     7695.55|\n",
      "+---------+------------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rising = 0.1 # 10% podwyżki\n",
    "spark.sql(f\"select firstname, lastname, salary, round(salary + salary * {rising},2) as after_rising from EMPLOYEE_DATA\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9547ba-b3c8-45eb-b3bf-6dc1bb277e26",
   "metadata": {},
   "source": [
    "# 2. Apache Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "406afae3-4e83-48fb-81fe-dcc3914231da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark_catalog'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentCatalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f997a28-415c-4de8-92f2-aff067346727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dla zrealizowania kolejnych przykładów dokonamy kilku modyfikacji pliku employee\n",
    "# 1. dodanie kolumny ID - indeksu\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df = df.withColumn(\"ID\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7114379e-7bdb-40f4-86de-72cbda09e957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+---+-------+\n",
      "| ID| firstname|    lastname|age| salary|\n",
      "+---+----------+------------+---+-------+\n",
      "|  0|  Wojciech|      Szczaw| 62|8132.22|\n",
      "|  1|  Zbigniew|      Wlotka| 66|8268.47|\n",
      "|  2|     Agata|Mieczykowski| 37|6116.28|\n",
      "|  3|  Zbigniew|  Wróblewski| 25|9377.25|\n",
      "|  4|      Adam|    Kowalski| 30|6995.95|\n",
      "|  5|     Marek|      Wlotka| 60|8800.12|\n",
      "|  6|  Wojciech|    Kowalski| 49| 8335.9|\n",
      "|  7|  Zbigniew|       Pysla| 37|5548.33|\n",
      "|  8| Katarzyna|Mieczykowski| 58| 7164.0|\n",
      "|  9|Aleksandra|    Barański| 68|7037.87|\n",
      "+---+----------+------------+---+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a32b4ca-cb7c-4bb6-b962-553a5ccab3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dokonamy podziału danych i zapisania w różnych formatach\n",
    "splits = df.randomSplit(weights=[0.3, 0.7], seed=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f01a992-977b-48a1-b0e9-fd7fd65342d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "(598911, 1401089)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].count(), splits[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a18b5-8ab2-4cb0-a9cd-856d420f8af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to dość dziwne zjawisko niezbyt równego podziału danych jest opisane w artykułach:\n",
    "# https://medium.com/udemy-engineering/pyspark-under-the-hood-randomsplit-and-sample-inconsistencies-examined-7c6ec62644bc\n",
    "# oraz\n",
    "# https://www.geeksforgeeks.org/pyspark-randomsplit-and-sample-methods/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2171365-4ec2-46e3-8026-db377b6e6daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# większa część trafi do nowej tymczasowej tabeli\n",
    "splits[1].createOrReplaceTempView(\"EMPLOYEE_DATA_SPLIT_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "409c9325-3843-4aeb-94b2-b8273ca74031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# a mniejsza do plików JSON\n",
    "splits[0].write.json('employee_data.json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6598b70f-9a9c-4621-be0d-3d0e73898506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./employee_data.json/part-00000-6eca5db4-562a-4683-9c46-703b41dda1c2-c000.json\n",
      "./employee_data.json/part-00001-6eca5db4-562a-4683-9c46-703b41dda1c2-c000.json\n"
     ]
    }
   ],
   "source": [
    "!ls ./employee_data.json/*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b20c433-5e1a-4c90-bfa9-0044700a20d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aby móc wykorzystać dane w przykładach ze złączaniem, zapiszemy jeszcze próbkę danych z głównej ramki\n",
    "# z identyfikatorami oraz dodatkową kolumną z podwyżką\n",
    "from pyspark.sql.functions import col, lit, round\n",
    "\n",
    "lucky_guys = spark.sql(\"select * from EMPLOYEE_DATA\").sample(0.01)\\\n",
    ".withColumn('rising', lit('10%')).withColumn('salary after rising', round(col('salary') * 1.1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0af6a6a6-e872-40ec-ad37-1d5c3e5efd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                24/11/21 19:22:18 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/11/21 19:22:18 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/11/21 19:22:18 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/11/21 19:22:18 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "# zapisujemy szczęściarzy do oddzielnej tabeli w hurtowni\n",
    "lucky_guys.write.mode('overwrite').saveAsTable(\"lucky_employees\", format='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2b63f82-991f-488e-ae18-c1361d8b5f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='lucky_employees', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='EMPLOYEE_DATA', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='EMPLOYEE_DATA_SPLIT_1', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ef3ff-090b-4d5d-9111-08e0be160767",
   "metadata": {},
   "source": [
    "## Złączenie danych z różnych źródeł danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "710a2860-b468-4de5-a2be-41d8eef5156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./metastore_db/lucky_employees/part-00000-06b810b3-4cd5-427a-af22-631c17981769-c000.snappy.parquet\n",
      "./metastore_db/lucky_employees/part-00001-06b810b3-4cd5-427a-af22-631c17981769-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls ./metastore_db/lucky_employees/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d83cdc60-89c2-473b-b5b0-e28371fb4251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 19:23:11 WARN ObjectStore: Failed to get database json, returning NoSuchObjectException\n",
      "24/11/21 19:23:11 WARN ObjectStore: Failed to get database parquet, returning NoSuchObjectException\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------+-------+------+-------------------+\n",
      "|    ID| firstname|    lastname| salary|rising|salary after rising|\n",
      "+------+----------+------------+-------+------+-------------------+\n",
      "|313329|   Wisława|      Wlotka|6660.97|   10%|            7327.07|\n",
      "|356910|     Marek|        Glut|8337.18|   10%|             9170.9|\n",
      "|460218|     Agata|        Glut|8289.55|   10%|            9118.51|\n",
      "|466630| Krzysztof|Mieczykowski|6149.86|   10%|            6764.85|\n",
      "|484580|     Marek|Mieczykowski|8407.88|   10%|            9248.67|\n",
      "|554314|     Agata|  Malinowski|7294.92|   10%|            8024.41|\n",
      "|596462|   Wisława|Mieczykowski|5817.28|   10%|            6399.01|\n",
      "|607470|Mieczysław|  Wróblewski|8101.26|   10%|            8911.39|\n",
      "|633612|Aleksandra|    Barański|6274.59|   10%|            6902.05|\n",
      "|652223|  Wojciech|  Malinowski| 8430.6|   10%|            9273.66|\n",
      "+------+----------+------------+-------+------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# przykład złączania danych na różnych źródłach danych\n",
    "# zapytanie SQL bezpośrednio na plikach - tutaj zapisanych wcześniej JSON-ach oraz parquet\n",
    "query = \"\"\"\n",
    "SELECT ed.ID, ed.firstname, ed.lastname, ed.salary, lucky.rising, lucky.`salary after rising`\n",
    "FROM json.`./employee_data.json/` as jtable \n",
    "join EMPLOYEE_DATA ed on jtable.ID=ed.ID \n",
    "join parquet.`./metastore_db/lucky_employees/` as lucky on ed.ID=lucky.ID\n",
    "\"\"\"\n",
    "df_from_json = spark.sql(query).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421113ee-2dad-45d7-9895-b9e27832ca8c",
   "metadata": {},
   "source": [
    "## Dzielenie danych na wiaderka (ang. buckets) i partycje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94b5ef36-bc20-4161-9854-54828b9719b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ten przykład pokazuje podział na 16 wiaderek danych bazując na podziale po kolumnie ID (tu używane jest hashowanie)\n",
    "# dane posortowane są w każdym buckecie po kolumnie salary\n",
    "# dane zapisywane są do hurtowni Hive, a informacje o zapisanych tam tabelach przechowywane są w\n",
    "# Hive metastore (domyślnie jest do baza danych Derby)\n",
    "df.write.bucketBy(16, 'ID').mode('overwrite').sortBy('salary').saveAsTable('employee_id_bucketed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7931993-c011-4243-9079-f625041d7832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metastore_db/employee_id_bucketed/part-00000-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00000.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00000-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00001.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00000-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00002.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00000-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00003.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00000-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00004.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00000-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00005.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00000-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00006.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00000-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00007.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00000-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00008.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00000-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00009.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00000-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00010.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00000-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00011.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00000-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00012.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00000-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00013.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00000-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00014.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00000-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00015.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00001-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00000.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00001-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00001.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00001-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00002.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00001-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00003.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00001-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00004.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00001-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00005.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00001-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00006.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00001-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00007.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00001-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00008.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00001-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00009.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00001-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00010.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00001-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00011.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00001-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00012.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00001-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00013.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00001-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00014.c000.snappy.parquet\n",
      "metastore_db/employee_id_bucketed/part-00001-bd54208b-d0a6-4dc7-aef5-f6d631accb7f_00015.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls metastore_db/employee_id_bucketed/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bbb93b5-8c19-4c2c-8a99-885ebc8fd675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------------+---+-------+\n",
      "|    ID|firstname|    lastname|age| salary|\n",
      "+------+---------+------------+---+-------+\n",
      "|272129|     Adam|      Szczaw| 60|3428.34|\n",
      "|817327| Wojciech|      Szczaw| 44|3448.94|\n",
      "|398890|    Marek|  Wróblewski| 28|4009.85|\n",
      "| 35636|Krzysztof|      Wlotka| 57|4066.73|\n",
      "| 33375|    Agata|      Wlotka| 56|4070.07|\n",
      "|  6427|     Adam|Mieczykowski| 32| 4083.1|\n",
      "|129098| Wojciech|    Barański| 58|4101.87|\n",
      "| 35177|    Marek|        Glut| 63| 4107.8|\n",
      "|181166|    Agata|      Szczaw| 67|4205.51|\n",
      "|484952|     Adam|      Wlotka| 27|4219.78|\n",
      "+------+---------+------------+---+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table('employee_id_bucketed').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52b16d1a-cdba-4f15-81f1-cbb8ba19f5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employee_id_bucketed', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='lucky_employees', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='EMPLOYEE_DATA', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='EMPLOYEE_DATA_SPLIT_1', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wypisanie tabeli\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53af17c6-5dbe-4698-b3cf-f8da7fec6422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usunięcie tabeli\n",
    "spark.sql('DROP TABLE employee_id_bucketed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1dd8ff0-0c9e-4784-8487-b117d720c10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# jeżeli dane, z którymi pracujemy zawierają stosunkowo niewiele różnorodnych wartości w danych kolumnach\n",
    "# lub filtrowanie i obliczenia często odbywają się na podgrupach danych to lepsze efekty uzyskamy\n",
    "# poprzez wykorzystanie możliwości partycjonowania tych danych, które to partycjonowanie\n",
    "# będzie również odzwierciedlone w fizycznej strukturze plików na dysku twardym w hurtowni danych\n",
    "\n",
    "# zobaczmy przykład poniżej\n",
    "\n",
    "df.write.partitionBy(\"lastname\").mode('overwrite').saveAsTable(\"employees_partitioned_lastname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d62c82ce-a15e-4a2e-8124-c0935cd00de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dobrym pomysłem jest też określenie ilości bucketów wynikających z danych w konkretnej kolumnie\n",
    "# i wykorzystanie do podziału\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.bucketBy.html\n",
    "buckets = spark.sql(\"select distinct firstname from EMPLOYEE_DATA\").count()\n",
    "buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8010dde3-bb0f-45a1-8c4b-4af47ec378ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'lastname=Barański'\t\t'lastname=Malinowski'\t 'lastname=Wlotka'\n",
      "'lastname=Brzęczyszczykiewicz'\t'lastname=Mieczykowski'  'lastname=Wróblewski'\n",
      "'lastname=Glut'\t\t\t'lastname=Pysla'\t  _SUCCESS\n",
      "'lastname=Kowalski'\t\t'lastname=Szczaw'\n"
     ]
    }
   ],
   "source": [
    "# widok danych podzielonych na partycję z punktu widzenia systemu plików\n",
    "!ls metastore_db/employees_partitioned_lastname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "240daead-7376-4a4b-b78f-4d417379b4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[lastname#19], functions=[avg(salary#21)])\n",
      "   +- Exchange hashpartitioning(lastname#19, 200), ENSURE_REQUIREMENTS, [plan_id=759]\n",
      "      +- HashAggregate(keys=[lastname#19], functions=[partial_avg(salary#21)])\n",
      "         +- Filter (isnotnull(lastname#19) AND (lastname#19 = Pysla))\n",
      "            +- FileScan csv [lastname#19,salary#21] Batched: false, DataFilters: [isnotnull(lastname#19), (lastname#19 = Pysla)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/lab_06/employee.csv], PartitionFilters: [], PushedFilters: [IsNotNull(lastname), EqualTo(lastname,Pysla)], ReadSchema: struct<lastname:string,salary:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.lastname == 'Pysla').groupby('lastname').agg({'salary': 'avg'}).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eaee06b2-b9af-484f-a86a-36e4f5699275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|lastname|      avg(salary)|\n",
      "+--------+-----------------+\n",
      "|   Pysla|7851.037662912827|\n",
      "+--------+-----------------+\n",
      "\n",
      "CPU times: user 13.3 ms, sys: 0 ns, total: 13.3 ms\n",
      "Wall time: 2.21 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.filter(df.lastname == 'Pysla').groupby('lastname').agg({'salary': 'avg'}).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec0177d7-51fb-4f9d-ae6b-6d8fac21f7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[lastname#493], functions=[avg(salary#492)])\n",
      "   +- Exchange hashpartitioning(lastname#493, 200), ENSURE_REQUIREMENTS, [plan_id=825]\n",
      "      +- HashAggregate(keys=[lastname#493], functions=[partial_avg(salary#492)])\n",
      "         +- FileScan parquet spark_catalog.default.employees_partitioned_lastname[salary#492,lastname#493] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/work-dir/lab_07/metastore_db/employees_partitioned_las..., PartitionFilters: [isnotnull(lastname#493), (lastname#493 = Pysla)], PushedFilters: [], ReadSchema: struct<salary:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select lastname, avg(salary) from employees_partitioned_lastname where lastname='Pysla' group by lastname\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9ae3e3d-228c-4a3f-ba04-b67c6ff29a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|lastname|      avg(salary)|\n",
      "+--------+-----------------+\n",
      "|   Pysla|7851.037662912827|\n",
      "+--------+-----------------+\n",
      "\n",
      "CPU times: user 11 ms, sys: 0 ns, total: 11 ms\n",
      "Wall time: 450 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"select lastname, avg(salary) from employees_partitioned_lastname where lastname='Pysla' group by lastname\").show(10)\n",
    "# jak widać operacja wykonała się szybciej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b40dd8-f5a3-408f-b411-17a023f267f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb59b10-f1b0-42df-b2b7-74eb084fa8a1",
   "metadata": {},
   "source": [
    "# Zadanie 1\n",
    "Wczytaj plik zamowienia za pomocą Sparka do dowolnego typu danych (RDD, Spark DataFrame) i dokonaj transformacji tak aby:\n",
    "\n",
    "* naprawić problemy z kodowaniem znaków (replace?) w kolumnie Sprzedawca\n",
    "* poprawić format danych w kolumnie Utarg\n",
    "* dodać odpowiednie typy danych\n",
    "* kolumna idZamowienia powinna być traktowana jako klucz (indeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c97ba9fc-7dbc-4bac-b81e-8325780881b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------------+------------+-----------+\n",
      "|  Kraj|Sprzedawca|Data_zamowienia|idZamowienia|      Utarg|\n",
      "+------+----------+---------------+------------+-----------+\n",
      "|Polska|  Kowalski|     16.07.2003|       10248|  440,00 z|\n",
      "|Polska|  Sowiäski|     10.07.2003|       10249|1 863,40 z|\n",
      "|Niemcy|   Peacock|     12.07.2003|       10250|1 552,60 z|\n",
      "+------+----------+---------------+------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 19:33:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Kraj, Sprzedawca, Data zamowienia, idZamowienia, Utarg\n",
      " Schema: Kraj, Sprzedawca, Data_zamowienia, idZamowienia, Utarg\n",
      "Expected: Data_zamowienia but found: Data zamowienia\n",
      "CSV file: file:///opt/spark/work-dir/lab_07/zamowienia.txt\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "from pyspark.sql.functions import regexp_replace, to_date\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Kraj\", StringType(), True),\n",
    "    StructField(\"Sprzedawca\", StringType(), True),\n",
    "    StructField(\"Data_zamowienia\", StringType(), True),\n",
    "    StructField(\"idZamowienia\", IntegerType(), True),\n",
    "    StructField(\"Utarg\", StringType(), True)\n",
    "])\n",
    "\n",
    "data = spark.read.csv(\"zamowienia.txt\", header=True, sep=\";\", schema=schema)\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93f04e74-5379-4a44-a1e8-b63827a497e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------------+------------+-----------+\n",
      "|  Kraj|Sprzedawca|Data_zamowienia|idZamowienia|      Utarg|\n",
      "+------+----------+---------------+------------+-----------+\n",
      "|Polska|  Kowalski|     16.07.2003|       10248|  440,00 z|\n",
      "|Polska|  Sowiaski|     10.07.2003|       10249|1 863,40 z|\n",
      "|Niemcy|   Peacock|     12.07.2003|       10250|1 552,60 z|\n",
      "+------+----------+---------------+------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 19:34:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Kraj, Sprzedawca, Data zamowienia, idZamowienia, Utarg\n",
      " Schema: Kraj, Sprzedawca, Data_zamowienia, idZamowienia, Utarg\n",
      "Expected: Data_zamowienia but found: Data zamowienia\n",
      "CSV file: file:///opt/spark/work-dir/lab_07/zamowienia.txt\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn(\"Sprzedawca\", regexp_replace(col(\"Sprzedawca\"), \"ä\", \"a\"))\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f983272f-1b37-4a64-bf0a-3332d2c79f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------------+------------+------+\n",
      "|  Kraj|Sprzedawca|Data_zamowienia|idZamowienia| Utarg|\n",
      "+------+----------+---------------+------------+------+\n",
      "|Polska|  Kowalski|     16.07.2003|       10248| 440.0|\n",
      "|Polska|  Sowiaski|     10.07.2003|       10249|1863.4|\n",
      "|Niemcy|   Peacock|     12.07.2003|       10250|1552.6|\n",
      "+------+----------+---------------+------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 19:35:33 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Kraj, Sprzedawca, Data zamowienia, idZamowienia, Utarg\n",
      " Schema: Kraj, Sprzedawca, Data_zamowienia, idZamowienia, Utarg\n",
      "Expected: Data_zamowienia but found: Data zamowienia\n",
      "CSV file: file:///opt/spark/work-dir/lab_07/zamowienia.txt\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn(\"Utarg\", regexp_replace(col(\"Utarg\"), r\"[^\\d,]\", \"\")) \\\n",
    "           .withColumn(\"Utarg\", regexp_replace(col(\"Utarg\"), \",\", \".\")) \\\n",
    "           .withColumn(\"Utarg\", col(\"Utarg\").cast(FloatType()))\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c55a8cdd-d621-4daf-ba13-b8d903966982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------------+------------+------+\n",
      "|  Kraj|Sprzedawca|Data_zamowienia|idZamowienia| Utarg|\n",
      "+------+----------+---------------+------------+------+\n",
      "|Polska|  Kowalski|     2003-07-16|       10248| 440.0|\n",
      "|Polska|  Sowiaski|     2003-07-10|       10249|1863.4|\n",
      "|Niemcy|   Peacock|     2003-07-12|       10250|1552.6|\n",
      "+------+----------+---------------+------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 19:36:12 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Kraj, Sprzedawca, Data zamowienia, idZamowienia, Utarg\n",
      " Schema: Kraj, Sprzedawca, Data_zamowienia, idZamowienia, Utarg\n",
      "Expected: Data_zamowienia but found: Data zamowienia\n",
      "CSV file: file:///opt/spark/work-dir/lab_07/zamowienia.txt\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn(\"Data_zamowienia\", to_date(col(\"Data_zamowienia\"), \"dd.MM.yyyy\"))\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2523a136-6f67-4f13-ad96-a39b368f9dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------------+-----+------+\n",
      "|  Kraj|Sprzedawca|Data_zamowienia|   id| Utarg|\n",
      "+------+----------+---------------+-----+------+\n",
      "|Polska|  Kowalski|     2003-07-16|10248| 440.0|\n",
      "|Polska|  Sowiaski|     2003-07-10|10249|1863.4|\n",
      "|Niemcy|   Peacock|     2003-07-12|10250|1552.6|\n",
      "+------+----------+---------------+-----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 19:36:42 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Kraj, Sprzedawca, Data zamowienia, idZamowienia, Utarg\n",
      " Schema: Kraj, Sprzedawca, Data_zamowienia, idZamowienia, Utarg\n",
      "Expected: Data_zamowienia but found: Data zamowienia\n",
      "CSV file: file:///opt/spark/work-dir/lab_07/zamowienia.txt\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumnRenamed(\"idZamowienia\", \"id\").withColumn(\"id\", col(\"id\").cast(StringType()))\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab57972-23cd-4400-a922-3273c7b8fd31",
   "metadata": {},
   "source": [
    "# Zadanie 2\n",
    " wykonaj wiaderkowanie danych i wykonaj dowolne zapytanie agregujące na tych danych vs. dane nie podzielone na wiaderka - porównaj czas\n",
    "* wykonaj partycjonowanie danych i zapisz je w formcie csv (wypróbuj partycjonowanie wg. kraju, nazwiska)\n",
    "* wykonaj zapytanie agregujące z filtrowanie po kolumnie, której użyłeś/-aś do partycjonowania na danych oryginalnych oraz partycjonowanych i porównaj czas wykonania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "326aae7d-1cae-4d01-9113-b06dd4004760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 19:41:12 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Kraj, Sprzedawca, Data zamowienia, idZamowienia, Utarg\n",
      " Schema: Kraj, Sprzedawca, Data_zamowienia, idZamowienia, Utarg\n",
      "Expected: Data_zamowienia but found: Data zamowienia\n",
      "CSV file: file:///opt/spark/work-dir/lab_07/zamowienia.txt\n",
      "                                                                                24/11/21 19:41:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Kraj, Sprzedawca, Data zamowienia, idZamowienia, Utarg\n",
      " Schema: Kraj, Sprzedawca, Data_zamowienia, idZamowienia, Utarg\n",
      "Expected: Data_zamowienia but found: Data zamowienia\n",
      "CSV file: file:///opt/spark/work-dir/lab_07/zamowienia.txt\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "output_path_kraj = \"output/partycje_kraj\"\n",
    "output_path_sprzedawca = \"output/partycje_sprzedawca\"\n",
    "\n",
    "data.write.partitionBy(\"Kraj\").csv(output_path_kraj, header=True, mode=\"overwrite\")\n",
    "data.write.partitionBy(\"Sprzedawca\").csv(output_path_sprzedawca, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8115549c-9d9f-43cf-b8b8-b2082ff8429f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+\n",
      "|  Kraj|     Suma_Utargu|\n",
      "+------+----------------+\n",
      "|Polska|333330.908657074|\n",
      "+------+----------------+\n",
      "\n",
      "CPU times: user 4.3 ms, sys: 7.71 ms, total: 12 ms\n",
      "Wall time: 407 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data.filter(col(\"Kraj\") == \"Polska\").groupBy(\"Kraj\").agg(sum(\"Utarg\").alias(\"Suma_Utargu\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91fd2657-b60e-4d44-9da1-188ab3778f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|Sprzedawca|      Suma_Utargu|\n",
      "+----------+-----------------+\n",
      "|  Kowalski|68792.25023651123|\n",
      "+----------+-----------------+\n",
      "\n",
      "CPU times: user 4.61 ms, sys: 7.82 ms, total: 12.4 ms\n",
      "Wall time: 303 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data.filter(col(\"Sprzedawca\") == \"Kowalski\").groupBy(\"Sprzedawca\").agg(sum(\"Utarg\").alias(\"Suma_Utargu\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9a042d48-f797-4df1-a2be-ec246c3bcae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|  Kraj|      Suma_Utargu|\n",
      "+------+-----------------+\n",
      "|Polska|333330.9099999999|\n",
      "+------+-----------------+\n",
      "\n",
      "CPU times: user 2.7 ms, sys: 11.3 ms, total: 14 ms\n",
      "Wall time: 731 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "partitioned_data_kraj = spark.read.csv(output_path_kraj, header=True, inferSchema=True)\n",
    "partitioned_data.filter(col(\"Kraj\") == \"Polska\").groupBy(\"Kraj\").agg(sum(\"Utarg\").alias(\"Suma_Utargu\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "593de6ef-5b49-429b-8fe8-59c1631626c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|Sprzedawca|Suma_Utargu|\n",
      "+----------+-----------+\n",
      "|  Kowalski|   68792.25|\n",
      "+----------+-----------+\n",
      "\n",
      "CPU times: user 11.2 ms, sys: 518 μs, total: 11.7 ms\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "partitioned_data_sprzedawca = spark.read.csv(output_path_sprzedawca, header=True, inferSchema=True)\n",
    "partitioned_data.filter(col(\"Sprzedawca\") == \"Kowalski\").groupBy(\"Sprzedawca\").agg(sum(\"Utarg\").alias(\"Suma_Utargu\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5a8f81-110d-46c9-85a6-76d303fdb914",
   "metadata": {},
   "source": [
    "# Zadanie 3\n",
    "Z danych wygeneruj 4 różne podzbiory próbek (wiersze wybrane losowo) i dodaj nową kolumnę w każdym z nich, np. w jednym stwórz kolumnę month wyciągając tylko miesiąc z daty, w drugim wartość netto zamówienia (przyjmując, że vat to 23%), w kolejnym zamień nazwisko na wielkie litery, w kolejnym dodaj kolumnę waluta z wartością PLN.\n",
    "\n",
    "Następnie zapisz każdy z tych zbiorów tak, że:\n",
    "* zbiór pierwszy to będzie tymczasowa tabela in-memory Sparka\n",
    "* zbiór drugi to plik(i) parquet\n",
    "* zbiór trzeci to plik(i) csv\n",
    "* zbiór czwarty to plik(i) json\n",
    "\n",
    "Wykonaj zapytanie złączające jak w przykładzie pobierając dane bezpośrednio z plików i wyświetl idZamowienia, Kraj, Sprzedawcę, Datę, Utarg oraz 4 nowo utworzone kolumny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e639cb18-2cc7-4ca4-a8e3-1d0fd1c626b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 19:49:00 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Kraj, Sprzedawca, Data zamowienia, idZamowienia, Utarg\n",
      " Schema: Kraj, Sprzedawca, Data_zamowienia, idZamowienia, Utarg\n",
      "Expected: Data_zamowienia but found: Data zamowienia\n",
      "CSV file: file:///opt/spark/work-dir/lab_07/zamowienia.txt\n",
      "24/11/21 19:49:00 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Kraj, Sprzedawca, Data zamowienia, idZamowienia, Utarg\n",
      " Schema: Kraj, Sprzedawca, Data_zamowienia, idZamowienia, Utarg\n",
      "Expected: Data_zamowienia but found: Data zamowienia\n",
      "CSV file: file:///opt/spark/work-dir/lab_07/zamowienia.txt\n",
      "24/11/21 19:49:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Kraj, Sprzedawca, Data zamowienia, idZamowienia, Utarg\n",
      " Schema: Kraj, Sprzedawca, Data_zamowienia, idZamowienia, Utarg\n",
      "Expected: Data_zamowienia but found: Data zamowienia\n",
      "CSV file: file:///opt/spark/work-dir/lab_07/zamowienia.txt\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, month, upper\n",
    "\n",
    "subset1 = data.sample(fraction=0.5, seed=42) \\\n",
    "    .withColumn(\"month\", month(col(\"Data_zamowienia\")))\n",
    "\n",
    "subset2 = data.sample(fraction=0.5, seed=42) \\\n",
    "    .withColumn(\"netto\", col(\"Utarg\") / 1.23)\n",
    "\n",
    "subset3 = data.sample(fraction=0.5, seed=42) \\\n",
    "    .withColumn(\"Sprzedawca\", upper(col(\"Sprzedawca\")))\n",
    "\n",
    "subset4 = data.sample(fraction=0.5, seed=42) \\\n",
    "    .withColumn(\"waluta\", lit(\"PLN\"))\n",
    "\n",
    "subset1.createOrReplaceTempView(\"subset1_table\")\n",
    "subset2.write.parquet(\"subset2.parquet\", mode=\"overwrite\")\n",
    "subset3.write.csv(\"subset3.csv\", mode=\"overwrite\", header=True)\n",
    "subset4.write.json(\"subset4.json\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d8c3c11-6e56-4ccd-ac75-e361378e3999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+---------------+-------+-----+------------------+----------------+------+\n",
      "|   id|  Kraj|Sprzedawca|Data_zamowienia|  Utarg|month|             netto|Sprzedawca_upper|waluta|\n",
      "+-----+------+----------+---------------+-------+-----+------------------+----------------+------+\n",
      "|10251|Niemcy| Leverling|     2003-07-15| 654.06|    7| 531.7560955760924|       LEVERLING|   PLN|\n",
      "|10255|Polska|     Dudek|     2003-07-15| 2490.5|    7|2024.7967479674796|           DUDEK|   PLN|\n",
      "|10258|Niemcy|   Davolio|     2003-07-23|1614.88|    7|1312.9105730754573|         DAVOLIO|   PLN|\n",
      "|10260|Niemcy|   Peacock|     2003-07-29|1504.65|    7|1223.2927027756607|         PEACOCK|   PLN|\n",
      "|10264|Polska|  Sowiaski|     2003-08-23| 695.62|    8| 565.5447114773882|        SOWIASKI|   PLN|\n",
      "|10266|Niemcy| Leverling|     2003-07-31| 346.56|    7| 281.7560955760925|       LEVERLING|   PLN|\n",
      "|10268|Niemcy|  Callahan|     2003-08-02| 1101.2|    8| 895.2845131478658|        CALLAHAN|   PLN|\n",
      "|10273|Niemcy| Leverling|     2003-08-12|2037.28|    8|  1656.32522707063|       LEVERLING|   PLN|\n",
      "|10274|Polska|  Sowiaski|     2003-08-16|  538.6|    8| 437.8861590129573|        SOWIASKI|   PLN|\n",
      "|10275|Niemcy|   Davolio|     2003-08-09| 291.84|    8|237.26828970560214|         DAVOLIO|   PLN|\n",
      "|10279|Niemcy|  Callahan|     2003-08-16|  351.0|    8| 285.3658536585366|        CALLAHAN|   PLN|\n",
      "|10284|Niemcy|   Peacock|     2003-08-27|1170.37|    8| 951.5203212334858|         PEACOCK|   PLN|\n",
      "|10285|Niemcy|   Davolio|     2003-08-26|1743.36|    8| 1417.365841749238|         DAVOLIO|   PLN|\n",
      "|10286|Niemcy|  Callahan|     2003-08-30| 3016.0|    8| 2452.032520325203|        CALLAHAN|   PLN|\n",
      "|10288|Niemcy|   Peacock|     2003-09-03|   80.1|    9| 65.12194997896025|         PEACOCK|   PLN|\n",
      "|10289|Polska|      King|     2003-08-28|  479.4|    8| 389.7560925987678|            KING|   PLN|\n",
      "|10296|Polska|  Sowiaski|     2003-09-11| 1050.6|    9| 854.1463216145834|        SOWIASKI|   PLN|\n",
      "|10298|Polska|  Sowiaski|     2003-09-11| 2645.0|    9|2150.4065040650407|        SOWIASKI|   PLN|\n",
      "|10301|Niemcy|  Callahan|     2003-09-17|  755.0|    9| 613.8211382113822|        CALLAHAN|   PLN|\n",
      "|10302|Niemcy|   Peacock|     2003-10-09| 2708.8|   10|2202.2764624618903|         PEACOCK|   PLN|\n",
      "+-----+------+----------+---------------+-------+-----+------------------+----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 19:52:24 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Kraj, Sprzedawca, Data zamowienia, idZamowienia, Utarg\n",
      " Schema: Kraj, Sprzedawca, Data_zamowienia, idZamowienia, Utarg\n",
      "Expected: Data_zamowienia but found: Data zamowienia\n",
      "CSV file: file:///opt/spark/work-dir/lab_07/zamowienia.txt\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "subset2_parquet = spark.read.parquet(\"subset2.parquet\")\n",
    "subset3_csv = spark.read.csv(\"subset3.csv\", header=True, inferSchema=True)\n",
    "subset4_json = spark.read.json(\"subset4.json\")\n",
    "\n",
    "subset2_parquet.createOrReplaceTempView(\"subset2_parquet\")\n",
    "subset3_csv.createOrReplaceTempView(\"subset3_csv\")\n",
    "subset4_json.createOrReplaceTempView(\"subset4_json\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    s1.id, \n",
    "    s1.Kraj, \n",
    "    s1.Sprzedawca,\n",
    "    s1.Data_zamowienia,\n",
    "    s1.Utarg,\n",
    "    s1.month,\n",
    "    s2.netto,\n",
    "    s3.Sprzedawca as Sprzedawca_upper,\n",
    "    s4.waluta\n",
    "FROM subset1_table s1\n",
    "JOIN subset2_parquet s2 ON s1.id = s2.id\n",
    "JOIN subset3_csv s3 ON s1.id = s3.id\n",
    "JOIN subset4_json s4 ON s1.id = s4.id\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
