{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T10:46:49.356161Z",
     "start_time": "2024-10-17T10:46:47.752949Z"
    }
   },
   "id": "5ec4d075c5182ae9",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1\n",
    "Poniżej zaprezentowany kod jest podejściem sekwencyjnym do wykonania zadania przetworzenia logów (parsowanie, konwersja daty) i w takiej formie nie można zrównoleglić go tak jak zostało to zaprezentowane na przykładzie powyżej. Przekształcenie łańcucha daty na obiekt datetime wymaga najpierw wykonania parsowania pliku. Zastanów się i spróbuj przerobić to rozwiązanie tak, aby możliwe było użycie wywołań dask delayed w celu zrównoleglenia części funkcji, np. parsowanie danych w celu pobrania wartości kolumn niezależnie (tylko jednej na raz). Dane końcowe możesz zapisać do dask DataFrame, a następnie do plików parquet.\n",
    "\n",
    "Aby zyskać jakieś porównanie między wersją sekwencyjną a zrównolegloną, dodaj odpowiednio dużo danych ze zbiorów podlinkowanych powyżej oraz zmierz czas obu rozwiązań."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d117db7d326b830"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-17T10:46:51.333843Z",
     "start_time": "2024-10-17T10:46:51.327277Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse(inp: str):\n",
    "    record = {}\n",
    "    \n",
    "    date_start = inp.find('[') + 1\n",
    "    date_end = inp.find(']')\n",
    "    date_s = slice(date_start, date_end)\n",
    "\n",
    "    level_start = inp.find('[', date_end) + 1\n",
    "    level_end = inp.find(']', level_start)\n",
    "    level_s = slice(level_start, level_end)\n",
    "\n",
    "    client_start = inp.find('[', level_end)\n",
    "    client_end = inp.find(']', client_start)\n",
    "\n",
    "    record[\"date\"] = inp[date_s]    \n",
    "    record[\"level\"] = inp[level_s]\n",
    "    record[\"client\"] = \"\" if client_start == -1 else inp[client_start + 8: client_end]\n",
    "    record[\"message\"] = inp[client_end + 2:] if record[\"client\"] else inp[level_end + 2:]\n",
    "    \n",
    "    return record\n",
    "\n",
    "def convert_date(rec):\n",
    "    rec[\"date\"] = datetime.strptime(rec[\"date\"], \"%a %b %d %H:%M:%S %Y\")\n",
    "\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['[Thu Jun 09 06:07:04 2005] [notice] LDAP: Built with OpenLDAP LDAP SDK',\n '[Thu Jun 09 06:07:04 2005] [notice] LDAP: SSL support unavailable',\n '[Thu Jun 09 06:07:04 2005] [notice] suEXEC mechanism enabled (wrapper: /usr/sbin/suexec)',\n '[Thu Jun 09 06:07:05 2005] [notice] Digest: generating secret for digest authentication ...',\n '[Thu Jun 09 06:07:05 2005] [notice] Digest: done',\n '[Thu Jun 09 06:07:05 2005] [notice] LDAP: Built with OpenLDAP LDAP SDK',\n '[Thu Jun 09 06:07:05 2005] [notice] LDAP: SSL support unavailable',\n '[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating channel.jni:jni ( channel.jni, jni)',\n \"[Thu Jun 09 06:07:05 2005] [error] config.update(): Can't create channel.jni:jni\",\n '[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating vm: ( vm, )']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag = db.read_text('data/Apache.log').filter(lambda x: 'script not found or unable to stat' not in x).map(lambda x: x.strip())\n",
    "lines = bag.compute()\n",
    "lines[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T10:46:53.532985Z",
     "start_time": "2024-10-17T10:46:51.973177Z"
    }
   },
   "id": "6a20652a57743aa1",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "datetime.timedelta(microseconds=955508)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "output = []\n",
    "for line in lines:\n",
    "    record = parse(line)\n",
    "    record = convert_date(record)\n",
    "    output.append(list(record.values()))\n",
    "    \n",
    "df = pd.DataFrame(output, columns=[\"date\", \"level\", \"client\", \"message\"])\n",
    "\n",
    "ddf = dd.from_pandas(df, npartitions=2)\n",
    "ddf.to_parquet('data/log_data1.parquet')\n",
    "\n",
    "datetime.now() - start"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T10:46:54.496960Z",
     "start_time": "2024-10-17T10:46:53.535501Z"
    }
   },
   "id": "93f91218d836fd3f",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "datetime.timedelta(seconds=15, microseconds=834998)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "delayed_records = [dask.delayed(parse)(line) for line in lines]\n",
    "delayed_converted = [dask.delayed(convert_date)(rec) for rec in delayed_records]\n",
    "\n",
    "final_result = dask.compute(*delayed_converted)\n",
    "df = pd.DataFrame(final_result, columns=[\"date\", \"level\", \"client\", \"message\"])\n",
    "\n",
    "ddf = dd.from_pandas(df, npartitions=2)\n",
    "ddf.to_parquet('data/log_data2.parquet')\n",
    "\n",
    "datetime.now() - start"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T10:47:15.700959Z",
     "start_time": "2024-10-17T10:46:59.857887Z"
    }
   },
   "id": "9138238d0b71f8ed",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2\n",
    "Wykorzystując przykłady zaprezentowane w labie wykonaj na danych people (możesz zmniejszyć lub zwiększyć ich wolumen w zależności od potrzeb) operację z użyciem Dask bag, która polegać będzie na przetworzeniu wszystkich plików i zapisaniu do plików o nazwie expired_{partition}.json rekordów, których ważność karty kredytowej wygasła (jest to wartość w formacie miesiąc/rok). Zapisując ustaw finalną liczbę plików na 10 jeżeli była inna. Możesz to zrobić poprzez zmianę ilość partycji dask bag (patrz link do API na początku laba)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4f772ec05e76be7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['D:/Projekty/BigDataAnalysis/lab_03/tasks/data/00.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/01.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/02.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/03.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/04.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/05.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/06.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/07.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/08.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/09.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/10.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/11.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/12.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/13.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/14.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/15.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/16.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/17.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/18.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/19.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/20.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/21.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/22.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/23.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/24.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/25.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/26.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/27.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/28.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/29.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/30.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/31.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/32.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/33.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/34.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/35.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/36.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/37.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/38.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/39.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/40.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/41.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/42.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/43.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/44.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/45.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/46.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/47.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/48.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/49.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/50.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/51.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/52.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/53.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/54.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/55.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/56.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/57.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/58.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/59.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/60.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/61.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/62.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/63.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/64.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/65.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/66.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/67.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/68.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/69.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/70.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/71.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/72.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/73.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/74.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/75.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/76.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/77.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/78.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/79.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/80.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/81.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/82.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/83.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/84.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/85.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/86.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/87.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/88.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/89.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/90.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/91.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/92.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/93.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/94.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/95.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/96.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/97.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/98.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/99.json']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dask.datasets.make_people(npartitions=100, records_per_partition=10000)\n",
    "b.map(json.dumps).to_textfiles(os.path.join('data', '*.json'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T10:48:50.946049Z",
     "start_time": "2024-10-17T10:48:26.417430Z"
    }
   },
   "id": "21889b104845004b",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def is_card_expired(record):\n",
    "    expiration = record['credit-card']['expiration-date']\n",
    "    exp_month, exp_year = map(int, expiration.split('/'))\n",
    "    current_year = datetime.now().year\n",
    "    current_month = datetime.now().month\n",
    "    exp_year += 2000\n",
    "    return (exp_year < current_year) or (exp_year == current_year and exp_month < current_month)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T10:56:16.331149Z",
     "start_time": "2024-10-17T10:56:16.326223Z"
    }
   },
   "id": "fc92289e1d48613a",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['D:/Projekty/BigDataAnalysis/lab_03/tasks/data/cards/expired_0.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/cards/expired_1.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/cards/expired_2.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/cards/expired_3.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/cards/expired_4.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/cards/expired_5.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/cards/expired_6.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/cards/expired_7.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/cards/expired_8.json',\n 'D:/Projekty/BigDataAnalysis/lab_03/tasks/data/cards/expired_9.json']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = db.read_text(os.path.join('data', '*.json')).map(json.loads)\n",
    "expired_cards = people.filter(is_card_expired)\n",
    "\n",
    "expired_cards = expired_cards.repartition(npartitions=10)\n",
    "expired_cards.map(json.dumps).to_textfiles(os.path.join('data/cards', 'expired_*.json'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T10:56:46.653943Z",
     "start_time": "2024-10-17T10:56:17.874564Z"
    }
   },
   "id": "35c2679da567c782",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3\n",
    "Wybierając z danych people dane tylko osób dorosłych (zaprezentowane w przykładach w tym labie) przechowaj je w obiekcie typu bag, a następnie zapisz je do dask dataframe za pomocą metody to_dataframe (pamiętaj o tym jaka jest aktualna struktura pojedynczego rekordu), a następnie zapisz do jednego pliku w formacie parquet."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f023c6e34ef8160c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def flatten_record(record):\n",
    "    flattened = {}\n",
    "    flattened['age'] = record['age']\n",
    "    flattened['name'] = ' '.join(record['name'])\n",
    "    flattened['occupation'] = record['occupation']\n",
    "    flattened['telephone'] = record['telephone']\n",
    "\n",
    "    flattened['address'] = record['address']['address']\n",
    "    flattened['city'] = record['address']['city']\n",
    "\n",
    "    flattened['credit_card_number'] = record['credit-card']['number']\n",
    "    flattened['credit_card_expiration'] = record['credit-card']['expiration-date']\n",
    "    \n",
    "    return flattened"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T11:02:00.282656Z",
     "start_time": "2024-10-17T11:02:00.275632Z"
    }
   },
   "id": "7dcaabac0388b5ca",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "adults = people.filter(lambda record: record['age'] >= 18).map(flatten_record)\n",
    "ddf = adults.to_dataframe()\n",
    "ddf.repartition(npartitions=1).to_parquet('data/adults/adults.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T11:12:13.439616Z",
     "start_time": "2024-10-17T11:12:00.079511Z"
    }
   },
   "id": "11b737bdd5ac5521",
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
